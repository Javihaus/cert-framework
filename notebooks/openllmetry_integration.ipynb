{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CERT + OpenLLMetry Integration\n",
        "\n",
        "This notebook demonstrates how to use **OpenLLMetry** to automatically send LLM traces to the **CERT Dashboard** for evaluation and monitoring.\n",
        "\n",
        "## Architecture\n",
        "```\n",
        "┌─────────────────────────────────────────────────────────────────┐\n",
        "│  Your Application (this notebook, or any app)                  │\n",
        "│                                                                 │\n",
        "│  Traceloop.init()  ← ONE LINE instruments everything           │\n",
        "│                                                                 │\n",
        "│  Automatically traces:                                          │\n",
        "│  ✓ OpenAI, Anthropic, Cohere, Gemini, Mistral, Bedrock         │\n",
        "│  ✓ LangChain, LlamaIndex, CrewAI, Haystack                     │\n",
        "│  ✓ Pinecone, Chroma, Qdrant, Weaviate                          │\n",
        "└────────────────────────┬────────────────────────────────────────┘\n",
        "                         │ OTLP Protocol (automatic)\n",
        "                         ▼\n",
        "              ┌──────────────────────┐\n",
        "              │   CERT Dashboard     │\n",
        "              │   /api/v1/traces     │\n",
        "              │                      │\n",
        "              │  • View all traces   │\n",
        "              │  • LLM Judge eval    │\n",
        "              │  • Cost tracking     │\n",
        "              │  • Performance       │\n",
        "              └──────────────────────┘\n",
        "```\n",
        "\n",
        "## No API Keys Shared!\n",
        "- Your API keys stay in YOUR environment\n",
        "- Only trace data (prompts, responses, metrics) is sent to CERT\n",
        "- CERT can be self-hosted for full privacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install OpenLLMetry and LLM SDKs\n",
        "!pip install -q traceloop-sdk\n",
        "!pip install -q openai anthropic\n",
        "!pip install -q langchain langchain-openai langchain-anthropic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configure CERT Endpoint\n",
        "\n",
        "Set the OTLP endpoint to point to your CERT dashboard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "# === CERT Dashboard Configuration ===\n",
        "# For local development:\n",
        "CERT_ENDPOINT = \"http://localhost:3000/api/v1/traces\"\n",
        "\n",
        "# For deployed CERT:\n",
        "# CERT_ENDPOINT = \"https://your-cert-dashboard.com/api/v1/traces\"\n",
        "\n",
        "# Configure OpenTelemetry to send to CERT\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_ENDPOINT\"] = CERT_ENDPOINT\n",
        "os.environ[\"OTEL_EXPORTER_OTLP_PROTOCOL\"] = \"http/json\"  # Use JSON for compatibility\n",
        "\n",
        "# Optional: Add authentication header\n",
        "# os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = \"Authorization=Bearer your-token\"\n",
        "\n",
        "print(f\"CERT endpoint configured: {CERT_ENDPOINT}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your LLM API keys (these stay in YOUR environment)\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
        "\n",
        "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
        "    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your Anthropic API key: ')\n",
        "\n",
        "print(\"API keys configured (stored locally only)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize OpenLLMetry\n",
        "\n",
        "**ONE LINE** to instrument all your LLM calls!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from traceloop.sdk import Traceloop\n",
        "\n",
        "# Initialize OpenLLMetry - this instruments EVERYTHING\n",
        "Traceloop.init(\n",
        "    app_name=\"cert-demo\",\n",
        "    disable_batch=True,  # Send traces immediately (for demo)\n",
        "    telemetry_enabled=False,  # Disable anonymous telemetry\n",
        ")\n",
        "\n",
        "print(\"✓ OpenLLMetry initialized\")\n",
        "print(\"  All LLM calls will now be traced and sent to CERT\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Make LLM Calls (Automatically Traced)\n",
        "\n",
        "Now any LLM call you make will be automatically traced and sent to CERT."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Direct OpenAI Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "# This call is AUTOMATICALLY traced by OpenLLMetry\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful financial analyst.\"},\n",
        "        {\"role\": \"user\", \"content\": \"What are the key metrics to evaluate a company's financial health?\"}\n",
        "    ],\n",
        "    temperature=0.7,\n",
        "    max_tokens=500\n",
        ")\n",
        "\n",
        "print(\"OpenAI Response:\")\n",
        "print(response.choices[0].message.content)\n",
        "print(f\"\\n→ Trace sent to CERT (tokens: {response.usage.total_tokens})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Direct Anthropic Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import anthropic\n",
        "\n",
        "claude = anthropic.Anthropic()\n",
        "\n",
        "# This call is AUTOMATICALLY traced by OpenLLMetry\n",
        "response = claude.messages.create(\n",
        "    model=\"claude-sonnet-4-5-20250929\",\n",
        "    max_tokens=500,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Explain the difference between P/E ratio and P/B ratio in 2 sentences.\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Claude Response:\")\n",
        "print(response.content[0].text)\n",
        "print(f\"\\n→ Trace sent to CERT (tokens: {response.usage.input_tokens + response.usage.output_tokens})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.3 LangChain Calls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Create chains with different models\n",
        "gpt_model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.3)\n",
        "claude_model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\", temperature=0.3)\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a concise financial advisor.\"),\n",
        "    (\"human\", \"{question}\")\n",
        "])\n",
        "\n",
        "# Chain 1: GPT-4o\n",
        "chain1 = prompt | gpt_model\n",
        "result1 = chain1.invoke({\"question\": \"What is dollar-cost averaging?\"})\n",
        "print(\"GPT-4o:\", result1.content[:200], \"...\")\n",
        "\n",
        "# Chain 2: Claude\n",
        "chain2 = prompt | claude_model\n",
        "result2 = chain2.invoke({\"question\": \"What is dollar-cost averaging?\"})\n",
        "print(\"\\nClaude:\", result2.content[:200], \"...\")\n",
        "\n",
        "print(\"\\n→ Both traces sent to CERT automatically!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.4 Custom Workflow with Decorators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from traceloop.sdk.decorators import workflow, task\n",
        "\n",
        "@workflow(name=\"financial-analysis\")\n",
        "def analyze_company(company_name: str):\n",
        "    \"\"\"Multi-step analysis workflow - each step is traced\"\"\"\n",
        "    \n",
        "    # Step 1: Get company overview (GPT)\n",
        "    overview = get_company_overview(company_name)\n",
        "    \n",
        "    # Step 2: Analyze risks (Claude)\n",
        "    risks = analyze_risks(company_name, overview)\n",
        "    \n",
        "    return {\n",
        "        \"company\": company_name,\n",
        "        \"overview\": overview,\n",
        "        \"risks\": risks\n",
        "    }\n",
        "\n",
        "@task(name=\"get-overview\")\n",
        "def get_company_overview(company: str) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Give a 2-sentence overview of {company} as a business.\"}],\n",
        "        max_tokens=100\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "@task(name=\"analyze-risks\")\n",
        "def analyze_risks(company: str, overview: str) -> str:\n",
        "    response = claude.messages.create(\n",
        "        model=\"claude-sonnet-4-5-20250929\",\n",
        "        max_tokens=150,\n",
        "        messages=[{\"role\": \"user\", \"content\": f\"Based on this overview of {company}: '{overview}', list 2 key business risks.\"}]\n",
        "    )\n",
        "    return response.content[0].text\n",
        "\n",
        "# Run the workflow\n",
        "result = analyze_company(\"Apple Inc.\")\n",
        "print(\"Analysis Result:\")\n",
        "print(f\"Company: {result['company']}\")\n",
        "print(f\"Overview: {result['overview']}\")\n",
        "print(f\"Risks: {result['risks']}\")\n",
        "print(\"\\n→ Complete workflow traced with parent-child spans!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. View Traces in CERT Dashboard\n",
        "\n",
        "Open your CERT dashboard to see all the traces we just sent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Fetch traces from CERT\n",
        "try:\n",
        "    response = requests.get(f\"{CERT_ENDPOINT}?llm_only=true&limit=10\")\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        print(f\"CERT Dashboard Stats:\")\n",
        "        print(f\"  Total traces: {data['stats']['total']}\")\n",
        "        print(f\"  LLM traces: {data['stats']['llmTraces']}\")\n",
        "        print(f\"  Total tokens: {data['stats']['totalTokens']:,}\")\n",
        "        print(f\"  By vendor: {data['stats']['byVendor']}\")\n",
        "        print(f\"\\nRecent LLM calls:\")\n",
        "        for trace in data['traces'][:5]:\n",
        "            if trace.get('llm'):\n",
        "                print(f\"  • {trace['llm']['vendor']}/{trace['llm']['model']} - {trace['llm']['totalTokens']} tokens\")\n",
        "    else:\n",
        "        print(f\"Could not fetch from CERT: {response.status_code}\")\n",
        "        print(\"Make sure your CERT dashboard is running!\")\n",
        "except requests.exceptions.ConnectionError:\n",
        "    print(\"Could not connect to CERT dashboard.\")\n",
        "    print(f\"Make sure it's running at: {CERT_ENDPOINT}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Alternative: Direct CERT SDK (No OpenLLMetry)\n",
        "\n",
        "If you prefer not to use OpenLLMetry, you can send traces directly to CERT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def send_to_cert(provider: str, model: str, input_text: str, output_text: str, \n",
        "                 prompt_tokens: int, completion_tokens: int, duration_ms: int):\n",
        "    \"\"\"Send a trace directly to CERT using the simple SDK format\"\"\"\n",
        "    \n",
        "    payload = {\n",
        "        \"traces\": [{\n",
        "            \"id\": f\"manual-{int(time.time() * 1000)}\",\n",
        "            \"provider\": provider,\n",
        "            \"model\": model,\n",
        "            \"input\": input_text,\n",
        "            \"output\": output_text,\n",
        "            \"promptTokens\": prompt_tokens,\n",
        "            \"completionTokens\": completion_tokens,\n",
        "            \"durationMs\": duration_ms,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }]\n",
        "    }\n",
        "    \n",
        "    response = requests.post(\n",
        "        CERT_ENDPOINT,\n",
        "        json=payload,\n",
        "        headers={\"Content-Type\": \"application/json\"}\n",
        "    )\n",
        "    \n",
        "    return response.json()\n",
        "\n",
        "# Example: Manual trace\n",
        "result = send_to_cert(\n",
        "    provider=\"openai\",\n",
        "    model=\"gpt-4o\",\n",
        "    input_text=\"What is the capital of France?\",\n",
        "    output_text=\"The capital of France is Paris.\",\n",
        "    prompt_tokens=12,\n",
        "    completion_tokens=8,\n",
        "    duration_ms=450\n",
        ")\n",
        "print(f\"Manual trace sent: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Demonstrated\n",
        "\n",
        "| Feature | How It Works |\n",
        "|---------|-------------|\n",
        "| **Auto-instrumentation** | `Traceloop.init()` - one line, instruments all LLM calls |\n",
        "| **Privacy** | API keys stay in your environment, only traces sent |\n",
        "| **Multi-provider** | Works with OpenAI, Anthropic, and any supported provider |\n",
        "| **Frameworks** | LangChain, LlamaIndex, and others auto-traced |\n",
        "| **Custom workflows** | Use `@workflow` and `@task` decorators for complex flows |\n",
        "| **Self-hosted** | CERT dashboard runs on your infrastructure |\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. Open CERT dashboard → View traces\n",
        "2. Run LLM Judge evaluations on collected traces\n",
        "3. Monitor costs and performance metrics\n",
        "4. Set up alerts for quality issues"
      ]
    }
  ]
}
