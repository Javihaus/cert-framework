{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Document Extraction Pipeline\n",
    "\n",
    "This notebook demonstrates a **two-agent pipeline** for extracting and analyzing financial documents:\n",
    "\n",
    "1. **Extractor Agent** (Claude Sonnet 4.5) - Reads and extracts key information from Apple's 10-K SEC filing\n",
    "2. **Report Agent** (GPT-4o) - Generates a brief analytical report based on the extracted data\n",
    "\n",
    "## Architecture\n",
    "```\n",
    "PDF Document → Extractor Agent (Claude) → Structured Data → Report Agent (GPT-4o) → Final Report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q langchain langchain-openai langchain-anthropic langchain-community\n!pip install -q langchain-text-splitters\n!pip install -q pypdf tiktoken\n!pip install -q pydantic"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API keys\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your Anthropic API key: ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Apple 10-K Filing\n",
    "\n",
    "We'll download Apple's latest 10-K filing from SEC EDGAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Apple's 10-K filing URL (FY2024)\n",
    "# You can replace this with any 10-K URL from SEC EDGAR\n",
    "SEC_10K_URL = \"https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\"\n",
    "\n",
    "# For this demo, we'll use a sample PDF approach\n",
    "# In production, you'd download the actual filing\n",
    "print(\"Note: For this demo, please upload Apple's 10-K PDF manually or provide the file path.\")\n",
    "print(\"You can download it from: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF file in Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload Apple's 10-K PDF file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "PDF_PATH = list(uploaded.keys())[0]\n",
    "print(f\"\\nUploaded: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load the PDF\nloader = PyPDFLoader(PDF_PATH)\ndocuments = loader.load()\n\nprint(f\"Loaded {len(documents)} pages from the PDF\")\n\n# Split into chunks for processing\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=4000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\nchunks = text_splitter.split_documents(documents)\nprint(f\"Split into {len(chunks)} chunks\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class FinancialMetrics(BaseModel):\n",
    "    \"\"\"Key financial metrics extracted from 10-K\"\"\"\n",
    "    total_revenue: Optional[str] = Field(description=\"Total net revenue/sales\")\n",
    "    net_income: Optional[str] = Field(description=\"Net income\")\n",
    "    gross_margin: Optional[str] = Field(description=\"Gross margin percentage\")\n",
    "    operating_income: Optional[str] = Field(description=\"Operating income\")\n",
    "    eps: Optional[str] = Field(description=\"Earnings per share (diluted)\")\n",
    "    total_assets: Optional[str] = Field(description=\"Total assets\")\n",
    "    total_debt: Optional[str] = Field(description=\"Total debt/liabilities\")\n",
    "    cash_and_equivalents: Optional[str] = Field(description=\"Cash and cash equivalents\")\n",
    "\n",
    "class BusinessSegments(BaseModel):\n",
    "    \"\"\"Revenue breakdown by segment\"\"\"\n",
    "    iphone_revenue: Optional[str] = Field(description=\"iPhone revenue\")\n",
    "    mac_revenue: Optional[str] = Field(description=\"Mac revenue\")\n",
    "    ipad_revenue: Optional[str] = Field(description=\"iPad revenue\")\n",
    "    wearables_revenue: Optional[str] = Field(description=\"Wearables, Home and Accessories revenue\")\n",
    "    services_revenue: Optional[str] = Field(description=\"Services revenue\")\n",
    "\n",
    "class RiskFactors(BaseModel):\n",
    "    \"\"\"Key risk factors identified\"\"\"\n",
    "    risks: List[str] = Field(description=\"List of key risk factors\")\n",
    "\n",
    "class ExtractedData(BaseModel):\n",
    "    \"\"\"Complete extracted data from 10-K filing\"\"\"\n",
    "    company_name: str = Field(description=\"Company name\")\n",
    "    fiscal_year: str = Field(description=\"Fiscal year of the filing\")\n",
    "    financial_metrics: FinancialMetrics\n",
    "    business_segments: BusinessSegments\n",
    "    risk_factors: RiskFactors\n",
    "    business_overview: str = Field(description=\"Brief business description\")\n",
    "    key_developments: List[str] = Field(description=\"Key developments or highlights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize Claude Sonnet 4.5 for extraction\n",
    "extractor_llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# Initialize GPT-4o for report generation\n",
    "report_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "print(\"  - Extractor: Claude Sonnet 4.5\")\n",
    "print(\"  - Report: GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent 1: Document Extractor (Claude Sonnet 4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Create the extraction prompt\n",
    "extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert financial analyst specializing in SEC filings analysis.\n",
    "Your task is to extract key information from Apple Inc.'s 10-K filing.\n",
    "\n",
    "Extract the following information and return it as valid JSON:\n",
    "- Company name and fiscal year\n",
    "- Key financial metrics (revenue, net income, margins, EPS, assets, debt, cash)\n",
    "- Revenue breakdown by business segment (iPhone, Mac, iPad, Wearables, Services)\n",
    "- Top 5 risk factors\n",
    "- Brief business overview (2-3 sentences)\n",
    "- Key developments or highlights (3-5 bullet points)\n",
    "\n",
    "If a specific value is not found in the provided text, use null.\n",
    "Always include the unit (e.g., \"$394.3 billion\" or \"45.2%\").\n",
    "\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{schema}\"\"\"),\n",
    "    (\"human\", \"\"\"Analyze the following sections from Apple's 10-K filing and extract the required information:\n",
    "\n",
    "---\n",
    "{document_text}\n",
    "---\n",
    "\n",
    "Return the extracted data as JSON:\"\"\")\n",
    "])\n",
    "\n",
    "# Create JSON parser\n",
    "json_parser = JsonOutputParser(pydantic_object=ExtractedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_document(chunks, llm, prompt):\n",
    "    \"\"\"\n",
    "    Extract information from document chunks using Claude.\n",
    "    Processes chunks in batches to stay within context limits.\n",
    "    \"\"\"\n",
    "    # Combine relevant chunks (focus on financial sections)\n",
    "    # In a production system, you'd use semantic search to find relevant sections\n",
    "    combined_text = \"\\n\\n---\\n\\n\".join([chunk.page_content for chunk in chunks[:30]])\n",
    "    \n",
    "    # Truncate if too long (Claude has 200K context but we want to be efficient)\n",
    "    if len(combined_text) > 100000:\n",
    "        combined_text = combined_text[:100000]\n",
    "    \n",
    "    print(f\"Processing {len(combined_text):,} characters of text...\")\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = prompt | llm\n",
    "    \n",
    "    # Run extraction\n",
    "    response = chain.invoke({\n",
    "        \"schema\": ExtractedData.model_json_schema(),\n",
    "        \"document_text\": combined_text\n",
    "    })\n",
    "    \n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AGENT 1: Document Extractor (Claude Sonnet 4.5)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExtracting information from 10-K filing...\")\n",
    "\n",
    "# Run extraction\n",
    "extraction_result = extract_from_document(chunks, extractor_llm, extraction_prompt)\n",
    "\n",
    "# Parse the JSON response\n",
    "try:\n",
    "    # Find JSON in the response\n",
    "    json_start = extraction_result.find('{')\n",
    "    json_end = extraction_result.rfind('}') + 1\n",
    "    json_str = extraction_result[json_start:json_end]\n",
    "    extracted_data = json.loads(json_str)\n",
    "    print(\"\\n✓ Extraction complete!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nWarning: Could not parse JSON response. Using raw response.\")\n",
    "    extracted_data = {\"raw_response\": extraction_result}\n",
    "\n",
    "# Display extracted data\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"EXTRACTED DATA:\")\n",
    "print(\"-\"*60)\n",
    "print(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent 2: Report Generator (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the report generation prompt\n",
    "report_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a senior financial analyst writing an executive briefing report.\n",
    "Your task is to create a concise, professional report based on extracted 10-K data.\n",
    "\n",
    "The report should:\n",
    "1. Be clear and actionable for executive leadership\n",
    "2. Highlight key financial performance metrics\n",
    "3. Identify trends and notable changes\n",
    "4. Summarize material risks\n",
    "5. Provide a brief outlook or recommendation\n",
    "\n",
    "Use professional financial language but keep it accessible.\n",
    "Format with clear sections and bullet points where appropriate.\"\"\"),\n",
    "    (\"human\", \"\"\"Based on the following extracted data from Apple Inc.'s 10-K filing, \n",
    "generate a concise executive briefing report:\n",
    "\n",
    "EXTRACTED DATA:\n",
    "{extracted_data}\n",
    "\n",
    "Generate a professional executive briefing report (approximately 500-700 words):\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"AGENT 2: Report Generator (GPT-4o)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nGenerating executive briefing report...\")\n",
    "\n",
    "# Create the report chain\n",
    "report_chain = report_prompt | report_llm\n",
    "\n",
    "# Generate the report\n",
    "report_response = report_chain.invoke({\n",
    "    \"extracted_data\": json.dumps(extracted_data, indent=2)\n",
    "})\n",
    "\n",
    "final_report = report_response.content\n",
    "\n",
    "print(\"\\n✓ Report generation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXECUTIVE BRIEFING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create output dictionary\n",
    "output = {\n",
    "    \"metadata\": {\n",
    "        \"source_document\": PDF_PATH,\n",
    "        \"extraction_model\": \"claude-sonnet-4-5-20250929\",\n",
    "        \"report_model\": \"gpt-4o\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"extracted_data\": extracted_data,\n",
    "    \"executive_report\": final_report\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_filename = f\"apple_10k_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_filename}\")\n",
    "\n",
    "# Download the file\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "Document: {PDF_PATH}\n",
    "Pages processed: {len(documents)}\n",
    "Chunks created: {len(chunks)}\n",
    "\n",
    "Agent 1 (Extractor):\n",
    "  - Model: Claude Sonnet 4.5\n",
    "  - Task: Extract structured data from 10-K filing\n",
    "  - Output: JSON with financial metrics, segments, risks\n",
    "\n",
    "Agent 2 (Report Generator):\n",
    "  - Model: GPT-4o\n",
    "  - Task: Generate executive briefing report\n",
    "  - Output: Professional narrative report\n",
    "\n",
    "Pipeline: PDF → Claude (Extract) → GPT-4o (Report) → Output\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "### Model Selection Rationale\n",
    "- **Claude Sonnet 4.5** for extraction: Excellent at structured data extraction, long context handling, and following complex schemas\n",
    "- **GPT-4o** for report generation: Strong narrative generation and professional writing capabilities\n",
    "\n",
    "### Production Considerations\n",
    "1. Add retry logic with exponential backoff for API calls\n",
    "2. Implement semantic chunking for better section identification\n",
    "3. Add validation layer to verify extracted data accuracy\n",
    "4. Use vector store for efficient retrieval of relevant sections\n",
    "5. Add cost tracking and token usage monitoring\n",
    "\n",
    "### Extending the Pipeline\n",
    "- Add a third agent for competitive analysis\n",
    "- Implement RAG for historical comparison\n",
    "- Add visualization agent for charts and graphs"
   ]
  }
 ]
}