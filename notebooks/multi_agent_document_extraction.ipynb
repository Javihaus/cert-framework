{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Multi-Agent Document Extraction Pipeline\n\nThis notebook demonstrates a **two-agent pipeline** for extracting and analyzing financial documents:\n\n1. **Extractor Agent** (Claude Sonnet 4.5) - Reads and extracts key information from Apple's 10-K SEC filing\n2. **Report Agent** (GPT-4o) - Generates a brief analytical report based on the extracted data\n\n## Architecture\n```\n                                   CERT Dashboard\n                                   (Monitoring & Eval)\n                                         â–²\n                                         â”‚ HTTP POST\n                                         â”‚ (direct traces)\n                                         â”‚\nPDF Document â†’ Extractor Agent â†’ Structured Data â†’ Report Agent â†’ Final Report\n               (Claude Sonnet)                      (GPT-4o)\n```\n\n## CERT Integration\nAll LLM calls send traces **directly** to your CERT Dashboard via simple HTTP POST:\n- No third-party services or API keys required\n- No complex telemetry dependencies\n- Just `requests` library - works everywhere\n- Real-time monitoring in CERT Dashboard"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q langchain langchain-openai langchain-anthropic langchain-community\n!pip install -q langchain-text-splitters\n!pip install -q pypdf tiktoken\n!pip install -q pydantic requests\n\nprint(\"âœ“ All packages installed (no telemetry dependencies needed!)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "# Set API keys\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass('Enter your OpenAI API key: ')\n",
    "\n",
    "if 'ANTHROPIC_API_KEY' not in os.environ:\n",
    "    os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your Anthropic API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "source": "# ============================================================\n# CERT Dashboard - Direct Trace Integration\n# ============================================================\n# Sends LLM call data directly to your CERT Dashboard\n# No third-party services, no extra API keys needed\n\nimport requests\nimport time\nimport uuid\nfrom datetime import datetime\n\n# âœ… Your deployed CERT Dashboard\nCERT_ENDPOINT = \"https://dashboard.cert-framework.com/api/v1/traces\"\n\nclass CERTTracer:\n    \"\"\"Lightweight tracer that sends LLM call data directly to CERT Dashboard\"\"\"\n    \n    def __init__(self, endpoint: str, app_name: str = \"default\"):\n        self.endpoint = endpoint\n        self.app_name = app_name\n        self.session_id = str(uuid.uuid4())[:8]\n        \n    def send_trace(self, trace_data: dict):\n        \"\"\"Send a trace to CERT Dashboard\"\"\"\n        try:\n            response = requests.post(\n                self.endpoint,\n                json={\"traces\": [trace_data]},\n                headers={\"Content-Type\": \"application/json\"},\n                timeout=10\n            )\n            if response.status_code == 200:\n                result = response.json()\n                print(f\"  âœ… Trace sent to CERT Dashboard (received: {result.get('received', 1)})\")\n                return True\n            else:\n                print(f\"  âš ï¸ CERT response: {response.status_code} - {response.text[:200]}\")\n                return False\n        except Exception as e:\n            print(f\"  âŒ Error: {e}\")\n            return False\n    \n    def trace_llm_call(self, provider: str, model: str, input_text: str, \n                       output_text: str, duration_ms: float, \n                       prompt_tokens: int = 0, completion_tokens: int = 0,\n                       status: str = \"ok\"):\n        \"\"\"Record an LLM call trace - uses exact format expected by CERT API\"\"\"\n        # This matches the parseCERTFormat() function in the API\n        trace = {\n            \"id\": f\"{self.session_id}-{str(uuid.uuid4())[:8]}\",\n            \"provider\": provider,           # e.g., \"anthropic\", \"openai\"\n            \"model\": model,                 # e.g., \"claude-sonnet-4-5-20250929\"\n            \"input\": input_text,\n            \"output\": output_text,\n            \"promptTokens\": prompt_tokens,\n            \"completionTokens\": completion_tokens,\n            \"durationMs\": round(duration_ms, 2),\n            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n            \"metadata\": {\n                \"app_name\": self.app_name,\n                \"session_id\": self.session_id\n            }\n        }\n        self.send_trace(trace)\n        return trace\n\n# Initialize the tracer\ncert_tracer = CERTTracer(\n    endpoint=CERT_ENDPOINT,\n    app_name=\"apple-10k-extraction\"\n)\n\nprint(\"=\" * 50)\nprint(\"CERT Dashboard Integration\")\nprint(\"=\" * 50)\nprint(f\"Endpoint: {CERT_ENDPOINT}\")\nprint(f\"Session:  {cert_tracer.session_id}\")\nprint(f\"\\nðŸ”— View traces at:\")\nprint(f\"   https://dashboard.cert-framework.com/operational/observability\")\nprint(\"=\" * 50)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download Apple 10-K Filing\n",
    "\n",
    "We'll download Apple's latest 10-K filing from SEC EDGAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Apple's 10-K filing URL (FY2024)\n",
    "# You can replace this with any 10-K URL from SEC EDGAR\n",
    "SEC_10K_URL = \"https://www.sec.gov/Archives/edgar/data/320193/000032019324000123/aapl-20240928.htm\"\n",
    "\n",
    "# For this demo, we'll use a sample PDF approach\n",
    "# In production, you'd download the actual filing\n",
    "print(\"Note: For this demo, please upload Apple's 10-K PDF manually or provide the file path.\")\n",
    "print(\"You can download it from: https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany&CIK=0000320193&type=10-K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF file in Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Please upload Apple's 10-K PDF file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Get the filename\n",
    "PDF_PATH = list(uploaded.keys())[0]\n",
    "print(f\"\\nUploaded: {PDF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process the PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load the PDF\nloader = PyPDFLoader(PDF_PATH)\ndocuments = loader.load()\n\nprint(f\"Loaded {len(documents)} pages from the PDF\")\n\n# Split into chunks for processing\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=4000,\n    chunk_overlap=200,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\n\nchunks = text_splitter.split_documents(documents)\nprint(f\"Split into {len(chunks)} chunks\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Data Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "class FinancialMetrics(BaseModel):\n",
    "    \"\"\"Key financial metrics extracted from 10-K\"\"\"\n",
    "    total_revenue: Optional[str] = Field(description=\"Total net revenue/sales\")\n",
    "    net_income: Optional[str] = Field(description=\"Net income\")\n",
    "    gross_margin: Optional[str] = Field(description=\"Gross margin percentage\")\n",
    "    operating_income: Optional[str] = Field(description=\"Operating income\")\n",
    "    eps: Optional[str] = Field(description=\"Earnings per share (diluted)\")\n",
    "    total_assets: Optional[str] = Field(description=\"Total assets\")\n",
    "    total_debt: Optional[str] = Field(description=\"Total debt/liabilities\")\n",
    "    cash_and_equivalents: Optional[str] = Field(description=\"Cash and cash equivalents\")\n",
    "\n",
    "class BusinessSegments(BaseModel):\n",
    "    \"\"\"Revenue breakdown by segment\"\"\"\n",
    "    iphone_revenue: Optional[str] = Field(description=\"iPhone revenue\")\n",
    "    mac_revenue: Optional[str] = Field(description=\"Mac revenue\")\n",
    "    ipad_revenue: Optional[str] = Field(description=\"iPad revenue\")\n",
    "    wearables_revenue: Optional[str] = Field(description=\"Wearables, Home and Accessories revenue\")\n",
    "    services_revenue: Optional[str] = Field(description=\"Services revenue\")\n",
    "\n",
    "class RiskFactors(BaseModel):\n",
    "    \"\"\"Key risk factors identified\"\"\"\n",
    "    risks: List[str] = Field(description=\"List of key risk factors\")\n",
    "\n",
    "class ExtractedData(BaseModel):\n",
    "    \"\"\"Complete extracted data from 10-K filing\"\"\"\n",
    "    company_name: str = Field(description=\"Company name\")\n",
    "    fiscal_year: str = Field(description=\"Fiscal year of the filing\")\n",
    "    financial_metrics: FinancialMetrics\n",
    "    business_segments: BusinessSegments\n",
    "    risk_factors: RiskFactors\n",
    "    business_overview: str = Field(description=\"Brief business description\")\n",
    "    key_developments: List[str] = Field(description=\"Key developments or highlights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Initialize Claude Sonnet 4.5 for extraction\n",
    "extractor_llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-5-20250929\",\n",
    "    temperature=0,\n",
    "    max_tokens=4096\n",
    ")\n",
    "\n",
    "# Initialize GPT-4o for report generation\n",
    "report_llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "print(\"Models initialized:\")\n",
    "print(\"  - Extractor: Claude Sonnet 4.5\")\n",
    "print(\"  - Report: GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent 1: Document Extractor (Claude Sonnet 4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Create the extraction prompt\n",
    "extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are an expert financial analyst specializing in SEC filings analysis.\n",
    "Your task is to extract key information from Apple Inc.'s 10-K filing.\n",
    "\n",
    "Extract the following information and return it as valid JSON:\n",
    "- Company name and fiscal year\n",
    "- Key financial metrics (revenue, net income, margins, EPS, assets, debt, cash)\n",
    "- Revenue breakdown by business segment (iPhone, Mac, iPad, Wearables, Services)\n",
    "- Top 5 risk factors\n",
    "- Brief business overview (2-3 sentences)\n",
    "- Key developments or highlights (3-5 bullet points)\n",
    "\n",
    "If a specific value is not found in the provided text, use null.\n",
    "Always include the unit (e.g., \"$394.3 billion\" or \"45.2%\").\n",
    "\n",
    "Return ONLY valid JSON matching this schema:\n",
    "{schema}\"\"\"),\n",
    "    (\"human\", \"\"\"Analyze the following sections from Apple's 10-K filing and extract the required information:\n",
    "\n",
    "---\n",
    "{document_text}\n",
    "---\n",
    "\n",
    "Return the extracted data as JSON:\"\"\")\n",
    "])\n",
    "\n",
    "# Create JSON parser\n",
    "json_parser = JsonOutputParser(pydantic_object=ExtractedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def extract_from_document(chunks, llm, prompt):\n    \"\"\"\n    Extract information from document chunks using Claude.\n    Automatically traces the call to CERT Dashboard.\n    \"\"\"\n    # Combine relevant chunks (focus on financial sections)\n    combined_text = \"\\n\\n---\\n\\n\".join([chunk.page_content for chunk in chunks[:30]])\n    \n    # Truncate if too long\n    if len(combined_text) > 100000:\n        combined_text = combined_text[:100000]\n    \n    print(f\"Processing {len(combined_text):,} characters of text...\")\n    \n    # Create the chain\n    chain = prompt | llm\n    \n    # Prepare input for tracing\n    input_text = f\"Extract financial data from Apple 10-K filing ({len(combined_text)} chars)\"\n    \n    # Run extraction with timing\n    start_time = time.time()\n    response = chain.invoke({\n        \"schema\": ExtractedData.model_json_schema(),\n        \"document_text\": combined_text\n    })\n    duration_ms = (time.time() - start_time) * 1000\n    \n    # Send trace to CERT Dashboard\n    cert_tracer.trace_llm_call(\n        provider=\"anthropic\",\n        model=\"claude-sonnet-4-5-20250929\",\n        input_text=input_text,\n        output_text=response.content[:500] + \"...\" if len(response.content) > 500 else response.content,\n        duration_ms=duration_ms,\n        prompt_tokens=response.usage_metadata.get('input_tokens', 0) if hasattr(response, 'usage_metadata') and response.usage_metadata else 0,\n        completion_tokens=response.usage_metadata.get('output_tokens', 0) if hasattr(response, 'usage_metadata') and response.usage_metadata else 0\n    )\n    \n    return response.content"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AGENT 1: Document Extractor (Claude Sonnet 4.5)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nExtracting information from 10-K filing...\")\n",
    "\n",
    "# Run extraction\n",
    "extraction_result = extract_from_document(chunks, extractor_llm, extraction_prompt)\n",
    "\n",
    "# Parse the JSON response\n",
    "try:\n",
    "    # Find JSON in the response\n",
    "    json_start = extraction_result.find('{')\n",
    "    json_end = extraction_result.rfind('}') + 1\n",
    "    json_str = extraction_result[json_start:json_end]\n",
    "    extracted_data = json.loads(json_str)\n",
    "    print(\"\\nâœ“ Extraction complete!\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"\\nWarning: Could not parse JSON response. Using raw response.\")\n",
    "    extracted_data = {\"raw_response\": extraction_result}\n",
    "\n",
    "# Display extracted data\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(\"EXTRACTED DATA:\")\n",
    "print(\"-\"*60)\n",
    "print(json.dumps(extracted_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Agent 2: Report Generator (GPT-4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the report generation prompt\n",
    "report_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a senior financial analyst writing an executive briefing report.\n",
    "Your task is to create a concise, professional report based on extracted 10-K data.\n",
    "\n",
    "The report should:\n",
    "1. Be clear and actionable for executive leadership\n",
    "2. Highlight key financial performance metrics\n",
    "3. Identify trends and notable changes\n",
    "4. Summarize material risks\n",
    "5. Provide a brief outlook or recommendation\n",
    "\n",
    "Use professional financial language but keep it accessible.\n",
    "Format with clear sections and bullet points where appropriate.\"\"\"),\n",
    "    (\"human\", \"\"\"Based on the following extracted data from Apple Inc.'s 10-K filing, \n",
    "generate a concise executive briefing report:\n",
    "\n",
    "EXTRACTED DATA:\n",
    "{extracted_data}\n",
    "\n",
    "Generate a professional executive briefing report (approximately 500-700 words):\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"AGENT 2: Report Generator (GPT-4o)\")\nprint(\"=\"*60)\nprint(\"\\nGenerating executive briefing report...\")\n\n# Create the report chain\nreport_chain = report_prompt | report_llm\n\n# Prepare input for tracing\nreport_input = json.dumps(extracted_data, indent=2)\n\n# Generate the report with timing\nstart_time = time.time()\nreport_response = report_chain.invoke({\n    \"extracted_data\": report_input\n})\nduration_ms = (time.time() - start_time) * 1000\n\nfinal_report = report_response.content\n\n# Send trace to CERT Dashboard\ncert_tracer.trace_llm_call(\n    provider=\"openai\",\n    model=\"gpt-4o\",\n    input_text=f\"Generate executive briefing from extracted Apple 10-K data\",\n    output_text=final_report[:500] + \"...\" if len(final_report) > 500 else final_report,\n    duration_ms=duration_ms,\n    prompt_tokens=report_response.usage_metadata.get('input_tokens', 0) if hasattr(report_response, 'usage_metadata') and report_response.usage_metadata else 0,\n    completion_tokens=report_response.usage_metadata.get('output_tokens', 0) if hasattr(report_response, 'usage_metadata') and report_response.usage_metadata else 0\n)\n\nprint(\"\\nâœ“ Report generation complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EXECUTIVE BRIEFING REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(final_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create output dictionary\n",
    "output = {\n",
    "    \"metadata\": {\n",
    "        \"source_document\": PDF_PATH,\n",
    "        \"extraction_model\": \"claude-sonnet-4-5-20250929\",\n",
    "        \"report_model\": \"gpt-4o\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    },\n",
    "    \"extracted_data\": extracted_data,\n",
    "    \"executive_report\": final_report\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_filename = f\"apple_10k_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_filename, 'w') as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_filename}\")\n",
    "\n",
    "# Download the file\n",
    "files.download(output_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pipeline Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n\n## Notes\n\n### Model Selection Rationale\n- **Claude Sonnet 4.5** for extraction: Excellent at structured data extraction, long context handling, and following complex schemas\n- **GPT-4o** for report generation: Strong narrative generation and professional writing capabilities\n\n### CERT Dashboard Integration\nThis notebook uses a **simple, direct approach** to send traces to CERT:\n- No OpenTelemetry, no Traceloop, no third-party services\n- Just HTTP POST with `requests` library\n- Traces include: model, vendor, input/output, duration, tokens\n- Works in any environment (Colab, local, cloud)\n\n**To connect from Colab to your local CERT Dashboard:**\n1. Install ngrok: `npm install -g ngrok`\n2. Run CERT locally: `cd dashboard && npm run dev`\n3. Expose with ngrok: `ngrok http 3000`\n4. Copy the ngrok URL to `CERT_ENDPOINT` in this notebook\n\n### Production Considerations\n1. Add retry logic with exponential backoff for API calls\n2. Implement semantic chunking for better section identification\n3. Add validation layer to verify extracted data accuracy\n4. Use vector store for efficient retrieval of relevant sections\n5. Deploy CERT dashboard for production observability\n\n### Extending the Pipeline\n- Add a third agent for competitive analysis\n- Implement RAG for historical comparison\n- Add visualization agent for charts and graphs\n- Use CERT evaluations to monitor output quality over time"
  }
 ]
}