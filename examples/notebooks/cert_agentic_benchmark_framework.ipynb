{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "benchmark-header"
      },
      "source": [
        "# CERT Framework for Agentic AI applications: Model Provider Benchmarking Suite\n",
        "\n",
        "**Consistency, Effect, Resilience, Trustworthiness Analysis**\n",
        "\n",
        "This notebook implements a reproducible benchmarking framework to evaluate and compare language models across multiple providers (Anthropic, OpenAI, Google, xAI) on key business dimensions.\n",
        "\n",
        "**Framework Version:** 1.0\n",
        "\n",
        "**Last Updated:** October 2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "table-of-contents"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Environment Setup](#environment-setup)\n",
        "2. [Configuration](#configuration)\n",
        "3. [Core Framework](#core-framework)\n",
        "4. [Run Benchmark](#run-benchmark)\n",
        "5. [Results Analysis](#results-analysis)\n",
        "6. [Export & Visualization](#export--visualization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "environment-setup"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-dependencies",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install anthropic openai google-generativeai requests python-dotenv sentence-transformers torch pandas numpy scipy scikit-learn matplotlib seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-libraries",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import warnings\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import asdict, dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"Environment setup complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Check GPU availability for embeddings\n",
        "try:\n",
        "    import torch\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    if device == \"cuda\":\n",
        "        print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"Running on CPU. Consider enabling GPU in Colab for faster embeddings.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error checking GPU: {e}\")\n",
        "    device = \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "configuration"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auth-setup",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Google Colab: Store API keys securely\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve API keys (stored in Google Colab secrets)\n",
        "API_KEYS = {}\n",
        "\n",
        "# Try to get API keys from Colab secrets\n",
        "providers_to_test = ['ANTHROPIC', 'OPENAI', 'GOOGLE', 'XAI']\n",
        "\n",
        "for provider in providers_to_test:\n",
        "    try:\n",
        "        api_key = userdata.get(f\"{provider}_API_KEY\")\n",
        "        if api_key:\n",
        "            API_KEYS[provider] = api_key\n",
        "            print(f\"Loaded {provider} API key\")\n",
        "        else:\n",
        "            print(f\"Skipping {provider}: API key not found in Colab secrets\")\n",
        "    except userdata.NotebookAccessError:\n",
        "        print(f\"Cannot access {provider} API key - not running in Colab or key not set\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {provider} API key: {e}\")\n",
        "\n",
        "print(f\"\\nConfigured providers: {list(API_KEYS.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "benchmark-config",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# BENCHMARK CONFIGURATION\n",
        "\n",
        "@dataclass\n",
        "class BenchmarkConfig:\n",
        "    \"\"\"Configuration for benchmark execution.\"\"\"\n",
        "\n",
        "    # Trial configurations\n",
        "    consistency_trials: int = 20\n",
        "    performance_trials: int = 15\n",
        "    coordination_trials: int = 15\n",
        "\n",
        "    # Model selection\n",
        "    providers: Dict[str, List[str]] = field(default_factory=lambda: {\n",
        "        'anthropic': ['claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20241022'],\n",
        "        'openai': ['gpt-4o-mini'],\n",
        "        'google': ['gemini-2.5-flash', 'gemini-2.0-flash'],\n",
        "        'xai': ['grok-2']\n",
        "    })\n",
        "\n",
        "    # Embedding model\n",
        "    embedding_model_name: str = 'all-MiniLM-L6-v2'\n",
        "\n",
        "    # API parameters\n",
        "    max_tokens: int = 1024\n",
        "    temperature: float = 0.7\n",
        "    timeout: int = 30\n",
        "\n",
        "    # Test prompts\n",
        "    consistency_prompt: str = (\n",
        "        \"Analyze the key factors in effective business strategy implementation. \"\n",
        "        \"Provide a concise, structured response.\"\n",
        "    )\n",
        "\n",
        "    performance_prompts: List[str] = field(default_factory=lambda: [\n",
        "        \"Analyze the key factors in business strategy\",\n",
        "        \"Evaluate the main considerations for project management\",\n",
        "        \"Assess the critical elements in organizational change\",\n",
        "        \"Identify the primary aspects of market analysis\",\n",
        "        \"Examine the essential components of risk assessment\"\n",
        "    ])\n",
        "\n",
        "    # Output configuration\n",
        "    output_dir: str = '/content/benchmark_results'\n",
        "    random_seed: int = 42\n",
        "\n",
        "    def __post_init__(self):\n",
        "        \"\"\"Validate configuration after initialization.\"\"\"\n",
        "        if self.consistency_trials < 10:\n",
        "            raise ValueError(\"consistency_trials must be >= 10\")\n",
        "        if self.performance_trials < 5:\n",
        "            raise ValueError(\"performance_trials must be >= 5\")\n",
        "\n",
        "        # Create output directory\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "        # Set random seed for reproducibility\n",
        "        np.random.seed(self.random_seed)\n",
        "\n",
        "\n",
        "# Instantiate configuration\n",
        "config = BenchmarkConfig()\n",
        "print(f\"Configuration loaded. Output directory: {config.output_dir}\")\n",
        "print(f\"Configured providers and models: {config.providers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "core-framework"
      },
      "source": [
        "## 3. Core Framework"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data-structures",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Data structures for storing results\n",
        "\n",
        "@dataclass\n",
        "class ConsistencyResult:\n",
        "    \"\"\"Results from consistency testing.\"\"\"\n",
        "    provider: str\n",
        "    model: str\n",
        "    consistency_score: float\n",
        "    mean_distance: float\n",
        "    std_distance: float\n",
        "    num_trials: int\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PerformanceResult:\n",
        "    \"\"\"Results from performance testing.\"\"\"\n",
        "    provider: str\n",
        "    model: str\n",
        "    mean_score: float\n",
        "    std_score: float\n",
        "    min_score: float\n",
        "    max_score: float\n",
        "    num_trials: int\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CoordinationResult:\n",
        "    \"\"\"Results from coordination testing.\"\"\"\n",
        "    provider: str\n",
        "    model: str\n",
        "    mean_performance: float\n",
        "    std_performance: float\n",
        "    num_trials: int\n",
        "    degradation_factor: float\n",
        "    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())\n",
        "\n",
        "\n",
        "print(\"Data structures defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "provider-interface",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Abstract base class for providers\n",
        "\n",
        "class ProviderInterface(ABC):\n",
        "    \"\"\"Abstract interface for language model providers.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, timeout: int = 30):\n",
        "        \"\"\"Initialize provider with API key.\n",
        "\n",
        "        Args:\n",
        "            api_key: API key for the provider\n",
        "            timeout: Request timeout in seconds\n",
        "        \"\"\"\n",
        "        self.api_key = api_key\n",
        "        self.timeout = timeout\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "    @abstractmethod\n",
        "    async def call_model(\n",
        "        self,\n",
        "        model: str,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Call the language model.\n",
        "\n",
        "        Args:\n",
        "            model: Model name/identifier\n",
        "            prompt: Input prompt\n",
        "            max_tokens: Maximum tokens in response\n",
        "            temperature: Sampling temperature\n",
        "\n",
        "        Returns:\n",
        "            Model response text\n",
        "\n",
        "        Raises:\n",
        "            Exception: If API call fails\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_provider_name(self) -> str:\n",
        "        \"\"\"Get provider name.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "print(\"Provider interface defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "provider-anthropic",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Anthropic provider implementation\n",
        "\n",
        "class AnthropicProvider(ProviderInterface):\n",
        "    \"\"\"Anthropic Claude provider.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, timeout: int = 30):\n",
        "        super().__init__(api_key, timeout)\n",
        "        try:\n",
        "            from anthropic import Anthropic\n",
        "            self.client = Anthropic(api_key=api_key)\n",
        "            self.logger.info(\"Anthropic client initialized\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"anthropic package not installed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize Anthropic client: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def call_model(\n",
        "        self,\n",
        "        model: str,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Call Claude model.\"\"\"\n",
        "        try:\n",
        "            message = self.client.messages.create(\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            response = message.content[0].text\n",
        "            self.logger.debug(f\"Claude {model} response: {len(response)} chars\")\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Anthropic API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_provider_name(self) -> str:\n",
        "        return \"anthropic\"\n",
        "\n",
        "\n",
        "print(\"Anthropic provider implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "provider-openai",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# OpenAI provider implementation\n",
        "\n",
        "class OpenAIProvider(ProviderInterface):\n",
        "    \"\"\"OpenAI GPT provider.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, timeout: int = 30):\n",
        "        super().__init__(api_key, timeout)\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            self.client = OpenAI(api_key=api_key)\n",
        "            self.logger.info(\"OpenAI client initialized\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"openai package not installed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize OpenAI client: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def call_model(\n",
        "        self,\n",
        "        model: str,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Call GPT model.\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            text = response.choices[0].message.content\n",
        "            self.logger.debug(f\"GPT {model} response: {len(text)} chars\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"OpenAI API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_provider_name(self) -> str:\n",
        "        return \"openai\"\n",
        "\n",
        "\n",
        "print(\"OpenAI provider implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "provider-google",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Google Gemini provider implementation\n",
        "\n",
        "class GoogleProvider(ProviderInterface):\n",
        "    \"\"\"Google Gemini provider.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, timeout: int = 30):\n",
        "        super().__init__(api_key, timeout)\n",
        "        try:\n",
        "            import google.generativeai as genai\n",
        "            genai.configure(api_key=api_key)\n",
        "            self.genai = genai\n",
        "            self.logger.info(\"Google Gemini client initialized\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"google-generativeai package not installed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize Google client: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def call_model(\n",
        "        self,\n",
        "        model: str,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Call Gemini model.\"\"\"\n",
        "        try:\n",
        "            model_obj = self.genai.GenerativeModel(model)\n",
        "            response = model_obj.generate_content(\n",
        "                prompt,\n",
        "                generation_config=self.genai.types.GenerationConfig(\n",
        "                    max_output_tokens=max_tokens,\n",
        "                    temperature=temperature\n",
        "                )\n",
        "            )\n",
        "            text = response.text\n",
        "            self.logger.debug(f\"Gemini {model} response: {len(text)} chars\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Google Gemini API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_provider_name(self) -> str:\n",
        "        return \"google\"\n",
        "\n",
        "\n",
        "print(\"Google provider implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "provider-xai",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# xAI Grok provider implementation\n",
        "\n",
        "class XAIProvider(ProviderInterface):\n",
        "    \"\"\"xAI Grok provider.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, timeout: int = 30):\n",
        "        super().__init__(api_key, timeout)\n",
        "        try:\n",
        "            from openai import OpenAI\n",
        "            # Grok uses OpenAI-compatible API\n",
        "            self.client = OpenAI(\n",
        "                api_key=api_key,\n",
        "                base_url=\"https://api.x.ai/v1\"\n",
        "            )\n",
        "            self.logger.info(\"xAI Grok client initialized\")\n",
        "        except ImportError:\n",
        "            raise ImportError(\"openai package not installed\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize xAI client: {e}\")\n",
        "            raise\n",
        "\n",
        "    async def call_model(\n",
        "        self,\n",
        "        model: str,\n",
        "        prompt: str,\n",
        "        max_tokens: int = 1024,\n",
        "        temperature: float = 0.7\n",
        "    ) -> str:\n",
        "        \"\"\"Call Grok model.\"\"\"\n",
        "        try:\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=model,\n",
        "                max_tokens=max_tokens,\n",
        "                temperature=temperature,\n",
        "                messages=[\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "            )\n",
        "            text = response.choices[0].message.content\n",
        "            self.logger.debug(f\"Grok {model} response: {len(text)} chars\")\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"xAI API error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def get_provider_name(self) -> str:\n",
        "        return \"xai\"\n",
        "\n",
        "\n",
        "print(\"xAI provider implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "benchmark-engine",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Main benchmarking engine\n",
        "\n",
        "class CERTBenchmarkEngine:\n",
        "    \"\"\"CERT Framework benchmarking engine.\n",
        "\n",
        "    Measures:\n",
        "    - Consistency: Behavioral reliability across trials\n",
        "    - Effect: Multi-agent coordination performance\n",
        "    - Resilience: Performance under stress\n",
        "    - Trustworthiness: Reliability and explicability\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: BenchmarkConfig, providers: Dict[str, ProviderInterface]):\n",
        "        \"\"\"Initialize benchmark engine.\n",
        "\n",
        "        Args:\n",
        "            config: Benchmark configuration\n",
        "            providers: Dictionary of initialized providers\n",
        "        \"\"\"\n",
        "        self.config = config\n",
        "        self.providers = providers\n",
        "        self.logger = logging.getLogger(self.__class__.__name__)\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.logger.info(f\"Loading embedding model: {config.embedding_model_name}\")\n",
        "        self.embedding_model = SentenceTransformer(\n",
        "            config.embedding_model_name,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Results storage\n",
        "        self.consistency_results: List[ConsistencyResult] = []\n",
        "        self.performance_results: List[PerformanceResult] = []\n",
        "        self.coordination_results: List[CoordinationResult] = []\n",
        "        self.execution_log: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _calculate_consistency(\n",
        "        self,\n",
        "        responses: List[str]\n",
        "    ) -> Tuple[float, float, float]:\n",
        "        \"\"\"Calculate semantic consistency from responses.\n",
        "\n",
        "        Args:\n",
        "            responses: List of model responses\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (consistency_score, mean_distance, std_distance)\n",
        "        \"\"\"\n",
        "        if len(responses) < 2:\n",
        "            return 1.0, 0.0, 0.0\n",
        "\n",
        "        # Filter empty responses\n",
        "        valid_responses = [r for r in responses if r and len(r.strip()) > 0]\n",
        "        if len(valid_responses) < 2:\n",
        "            return 0.0, np.inf, 0.0\n",
        "\n",
        "        # Encode responses\n",
        "        embeddings = self.embedding_model.encode(\n",
        "            valid_responses,\n",
        "            show_progress_bar=False,\n",
        "            convert_to_tensor=False\n",
        "        )\n",
        "\n",
        "        # Calculate pairwise distances\n",
        "        distances = pdist(embeddings, metric='cosine')\n",
        "\n",
        "        if len(distances) == 0:\n",
        "            return 1.0, 0.0, 0.0\n",
        "\n",
        "        mean_distance = np.mean(distances)\n",
        "        std_distance = np.std(distances)\n",
        "\n",
        "        # Consistency: 1 - (std_dev / mean)\n",
        "        if mean_distance == 0:\n",
        "            consistency = 1.0\n",
        "        else:\n",
        "            consistency = max(0.0, 1.0 - (std_distance / mean_distance))\n",
        "\n",
        "        return consistency, mean_distance, std_distance\n",
        "\n",
        "    def _score_response(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        response: str\n",
        "    ) -> float:\n",
        "        \"\"\"Score response quality (0-1).\n",
        "\n",
        "        Evaluates:\n",
        "        - Semantic relevance to prompt\n",
        "        - Response length (completeness)\n",
        "        - Presence of structured content\n",
        "        \"\"\"\n",
        "        if not response or len(response.strip()) < 10:\n",
        "            return 0.0\n",
        "\n",
        "        try:\n",
        "            # Semantic relevance (50%)\n",
        "            prompt_embedding = self.embedding_model.encode(prompt, show_progress_bar=False)\n",
        "            response_embedding = self.embedding_model.encode(response, show_progress_bar=False)\n",
        "\n",
        "            relevance = float(np.dot(prompt_embedding, response_embedding) /\n",
        "                            (np.linalg.norm(prompt_embedding) * np.linalg.norm(response_embedding)))\n",
        "            relevance = max(0.0, min(1.0, (relevance + 1) / 2))  # Normalize [-1, 1] to [0, 1]\n",
        "\n",
        "            # Completeness (30%): based on response length\n",
        "            word_count = len(response.split())\n",
        "            completeness = min(1.0, word_count / 200)  # 200 words = excellent\n",
        "\n",
        "            # Structure (20%): presence of bullets, numbering, paragraphs\n",
        "            has_structure = 0.5\n",
        "            if '.' in response or '\\n' in response or ':' in response:\n",
        "                has_structure = 1.0\n",
        "\n",
        "            # Weighted score\n",
        "            score = (relevance * 0.5 + completeness * 0.3 + has_structure * 0.2)\n",
        "\n",
        "            return float(score)\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Error scoring response: {e}\")\n",
        "            return 0.5  # Default score\n",
        "\n",
        "    async def test_consistency(\n",
        "        self,\n",
        "        provider_name: str,\n",
        "        model: str\n",
        "    ) -> Optional[ConsistencyResult]:\n",
        "        \"\"\"Test model consistency.\n",
        "\n",
        "        Args:\n",
        "            provider_name: Provider identifier\n",
        "            model: Model name\n",
        "\n",
        "        Returns:\n",
        "            ConsistencyResult or None if failed\n",
        "        \"\"\"\n",
        "        if provider_name not in self.providers:\n",
        "            self.logger.error(f\"Provider {provider_name} not available\")\n",
        "            return None\n",
        "\n",
        "        provider = self.providers[provider_name]\n",
        "        responses = []\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"Testing consistency: {provider_name}/{model} \"\n",
        "            f\"({self.config.consistency_trials} trials)\"\n",
        "        )\n",
        "\n",
        "        for trial in range(self.config.consistency_trials):\n",
        "            try:\n",
        "                response = await provider.call_model(\n",
        "                    model,\n",
        "                    self.config.consistency_prompt,\n",
        "                    max_tokens=self.config.max_tokens,\n",
        "                    temperature=self.config.temperature\n",
        "                )\n",
        "                responses.append(response)\n",
        "\n",
        "                if (trial + 1) % 5 == 0:\n",
        "                    self.logger.info(f\"  Completed trial {trial + 1}/{self.config.consistency_trials}\")\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Trial {trial + 1} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not responses:\n",
        "            self.logger.error(f\"No successful responses for {provider_name}/{model}\")\n",
        "            return None\n",
        "\n",
        "        consistency, mean_dist, std_dist = self._calculate_consistency(responses)\n",
        "\n",
        "        result = ConsistencyResult(\n",
        "            provider=provider_name,\n",
        "            model=model,\n",
        "            consistency_score=consistency,\n",
        "            mean_distance=mean_dist,\n",
        "            std_distance=std_dist,\n",
        "            num_trials=len(responses)\n",
        "        )\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"Consistency: {consistency:.3f} \"\n",
        "            f\"(mean_dist={mean_dist:.3f}, std_dist={std_dist:.3f})\"\n",
        "        )\n",
        "\n",
        "        self.consistency_results.append(result)\n",
        "        return result\n",
        "\n",
        "    async def test_performance(\n",
        "        self,\n",
        "        provider_name: str,\n",
        "        model: str\n",
        "    ) -> Optional[PerformanceResult]:\n",
        "        \"\"\"Test model performance.\n",
        "\n",
        "        Args:\n",
        "            provider_name: Provider identifier\n",
        "            model: Model name\n",
        "\n",
        "        Returns:\n",
        "            PerformanceResult or None if failed\n",
        "        \"\"\"\n",
        "        if provider_name not in self.providers:\n",
        "            self.logger.error(f\"Provider {provider_name} not available\")\n",
        "            return None\n",
        "\n",
        "        provider = self.providers[provider_name]\n",
        "        scores = []\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"Testing performance: {provider_name}/{model} \"\n",
        "            f\"({self.config.performance_trials} trials)\"\n",
        "        )\n",
        "\n",
        "        for trial in range(self.config.performance_trials):\n",
        "            prompt = self.config.performance_prompts[\n",
        "                trial % len(self.config.performance_prompts)\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                response = await provider.call_model(\n",
        "                    model,\n",
        "                    prompt,\n",
        "                    max_tokens=self.config.max_tokens,\n",
        "                    temperature=self.config.temperature\n",
        "                )\n",
        "                score = self._score_response(prompt, response)\n",
        "                scores.append(score)\n",
        "\n",
        "                if (trial + 1) % 5 == 0:\n",
        "                    self.logger.info(\n",
        "                        f\"  Completed trial {trial + 1}/{self.config.performance_trials} \"\n",
        "                        f\"(avg score: {np.mean(scores):.3f})\"\n",
        "                    )\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Trial {trial + 1} failed: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not scores:\n",
        "            self.logger.error(f\"No successful scores for {provider_name}/{model}\")\n",
        "            return None\n",
        "\n",
        "        result = PerformanceResult(\n",
        "            provider=provider_name,\n",
        "            model=model,\n",
        "            mean_score=float(np.mean(scores)),\n",
        "            std_score=float(np.std(scores)),\n",
        "            min_score=float(np.min(scores)),\n",
        "            max_score=float(np.max(scores)),\n",
        "            num_trials=len(scores)\n",
        "        )\n",
        "\n",
        "        self.logger.info(\n",
        "            f\"Performance: mean={result.mean_score:.3f}, \"\n",
        "            f\"std={result.std_score:.3f}\"\n",
        "        )\n",
        "\n",
        "        self.performance_results.append(result)\n",
        "        return result\n",
        "\n",
        "    async def run_full_benchmark(\n",
        "        self,\n",
        "        test_consistency: bool = True,\n",
        "        test_performance: bool = True,\n",
        "        test_coordination: bool = False\n",
        "    ) -> Dict[str, List[Any]]:\n",
        "        \"\"\"Run complete benchmark suite.\n",
        "\n",
        "        Args:\n",
        "            test_consistency: Whether to run consistency tests\n",
        "            test_performance: Whether to run performance tests\n",
        "            test_coordination: Whether to run coordination tests\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all results\n",
        "        \"\"\"\n",
        "        start_time = datetime.now()\n",
        "        self.logger.info(\"Starting full benchmark suite\")\n",
        "\n",
        "        # Iterate over all configured providers and models\n",
        "        for provider_name, models in self.config.providers.items():\n",
        "            if provider_name not in self.providers:\n",
        "                self.logger.warning(f\"Provider {provider_name} not available, skipping\")\n",
        "                continue\n",
        "\n",
        "            for model in models:\n",
        "                self.logger.info(f\"\\nTesting {provider_name}/{model}\")\n",
        "\n",
        "                if test_consistency:\n",
        "                    await self.test_consistency(provider_name, model)\n",
        "\n",
        "                if test_performance:\n",
        "                    await self.test_performance(provider_name, model)\n",
        "\n",
        "        end_time = datetime.now()\n",
        "        duration = (end_time - start_time).total_seconds()\n",
        "\n",
        "        self.logger.info(f\"Benchmark completed in {duration:.1f} seconds\")\n",
        "\n",
        "        return {\n",
        "            'consistency': self.consistency_results,\n",
        "            'performance': self.performance_results,\n",
        "            'coordination': self.coordination_results,\n",
        "            'duration_seconds': duration,\n",
        "            'start_time': start_time.isoformat(),\n",
        "            'end_time': end_time.isoformat()\n",
        "        }\n",
        "\n",
        "\n",
        "print(\"Benchmark engine implemented.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "run-benchmark"
      },
      "source": [
        "## 4. Run Benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize-providers",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize providers based on available API keys\n",
        "\n",
        "provider_map = {\n",
        "    'ANTHROPIC': AnthropicProvider,\n",
        "    'OPENAI': OpenAIProvider,\n",
        "    'GOOGLE': GoogleProvider,\n",
        "    'XAI': XAIProvider\n",
        "}\n",
        "\n",
        "initialized_providers = {}\n",
        "\n",
        "for provider_key, provider_class in provider_map.items():\n",
        "    if provider_key in API_KEYS:\n",
        "        try:\n",
        "            provider_name = provider_key.lower()\n",
        "            provider_instance = provider_class(\n",
        "                api_key=API_KEYS[provider_key],\n",
        "                timeout=30\n",
        "            )\n",
        "            initialized_providers[provider_name] = provider_instance\n",
        "            print(f\"Initialized {provider_name} provider\")\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to initialize {provider_key}: {e}\")\n",
        "    else:\n",
        "        print(f\"Skipping {provider_key}: API key not available\")\n",
        "\n",
        "if not initialized_providers:\n",
        "    raise RuntimeError(\n",
        "        \"No providers initialized. Please add API keys to Colab secrets: \"\n",
        "        \"ANTHROPIC_API_KEY, OPENAI_API_KEY, GOOGLE_API_KEY, XAI_API_KEY\"\n",
        "    )\n",
        "\n",
        "print(f\"\\nProviders ready: {list(initialized_providers.keys())}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "customize-benchmark",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# CUSTOMIZE BENCHMARK CONFIGURATION\n",
        "\n",
        "# You can modify these settings before running the benchmark\n",
        "\n",
        "# Option 1: Test all available models\n",
        "config_all_models = BenchmarkConfig(\n",
        "    consistency_trials=20,\n",
        "    performance_trials=15,\n",
        "    providers={\n",
        "        'anthropic': ['claude-3-5-haiku-20241022'],\n",
        "        'openai': ['gpt-4o-mini'],\n",
        "        'google': ['gemini-2.5-flash'],\n",
        "        'xai': ['grok-2']\n",
        "    }\n",
        ")\n",
        "\n",
        "# Option 2: Quick test (fewer trials)\n",
        "config_quick = BenchmarkConfig(\n",
        "    consistency_trials=5,\n",
        "    performance_trials=5,\n",
        "    providers={\n",
        "        'anthropic': ['claude-3-5-haiku-20241022'],\n",
        "        'openai': ['gpt-4o-mini']\n",
        "    }\n",
        ")\n",
        "\n",
        "# Option 3: Comprehensive test\n",
        "config_comprehensive = BenchmarkConfig(\n",
        "    consistency_trials=50,\n",
        "    performance_trials=30,\n",
        "    providers={\n",
        "        'anthropic': ['claude-3-5-haiku-20241022', 'claude-3-5-sonnet-20241022'],\n",
        "        'openai': ['gpt-4o-mini'],\n",
        "        'google': ['gemini-2.5-flash', 'gemini-2.0-flash'],\n",
        "        'xai': ['grok-2']\n",
        "    }\n",
        ")\n",
        "\n",
        "# SELECT CONFIGURATION\n",
        "# Change this to config_quick, config_all_models, or config_comprehensive\n",
        "selected_config = config_all_models\n",
        "\n",
        "print(f\"Selected configuration: {selected_config}\")\n",
        "print(f\"Providers to test: {selected_config.providers}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "execute-benchmark",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize benchmark engine\n",
        "\n",
        "engine = CERTBenchmarkEngine(\n",
        "    config=selected_config,\n",
        "    providers=initialized_providers\n",
        ")\n",
        "\n",
        "print(\"Benchmark engine initialized\")\n",
        "print(f\"Output directory: {selected_config.output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run-benchmark-async",
        "tags": [],
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Run the benchmark\n",
        "\n",
        "results = await engine.run_full_benchmark(\n",
        "    test_consistency=True,\n",
        "    test_performance=True,\n",
        "    test_coordination=False\n",
        ")\n",
        "\n",
        "print(\"\\nBenchmark execution complete.\")\n",
        "print(f\"Total duration: {results['duration_seconds']:.1f} seconds\")\n",
        "print(f\"Consistency tests: {len(results['consistency'])}\")\n",
        "print(f\"Performance tests: {len(results['performance'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results-analysis"
      },
      "source": [
        "## 5. Results Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-dataframes",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Convert results to DataFrames for analysis\n",
        "\n",
        "consistency_df = pd.DataFrame([\n",
        "    asdict(r) for r in results['consistency']\n",
        "])\n",
        "\n",
        "performance_df = pd.DataFrame([\n",
        "    asdict(r) for r in results['performance']\n",
        "])\n",
        "\n",
        "# Display summary statistics\n",
        "print(\"CONSISTENCY RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(consistency_df[['provider', 'model', 'consistency_score', 'num_trials']].to_string(index=False))\n",
        "\n",
        "print(\"\\nPERFORMANCE RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(performance_df[['provider', 'model', 'mean_score', 'std_score', 'num_trials']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "statistical-analysis",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Statistical analysis\n",
        "\n",
        "if len(consistency_df) > 1:\n",
        "    print(\"CONSISTENCY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    consistency_summary = consistency_df.groupby('provider').agg({\n",
        "        'consistency_score': ['mean', 'std', 'min', 'max']\n",
        "    }).round(3)\n",
        "    print(consistency_summary)\n",
        "\n",
        "if len(performance_df) > 1:\n",
        "    print(\"\\nPERFORMANCE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    performance_summary = performance_df.groupby('provider').agg({\n",
        "        'mean_score': ['mean', 'std', 'min', 'max']\n",
        "    }).round(3)\n",
        "    print(performance_summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "combined-analysis",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Combine consistency and performance for overall comparison\n",
        "\n",
        "if len(consistency_df) > 0 and len(performance_df) > 0:\n",
        "    comparison_df = consistency_df[['provider', 'model', 'consistency_score']].copy()\n",
        "\n",
        "    # Join with performance scores\n",
        "    perf_merged = performance_df[['provider', 'model', 'mean_score']].copy()\n",
        "    perf_merged.columns = ['provider', 'model', 'performance_score']\n",
        "\n",
        "    comparison_df = comparison_df.merge(\n",
        "        perf_merged,\n",
        "        on=['provider', 'model'],\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # Normalize scores to 0-100 scale for readability\n",
        "    comparison_df['consistency_normalized'] = (comparison_df['consistency_score'] * 100).round(1)\n",
        "    comparison_df['performance_normalized'] = (comparison_df['performance_score'] * 100).round(1)\n",
        "\n",
        "    print(\"COMBINED COMPARISON\")\n",
        "    print(\"=\"*80)\n",
        "    print(comparison_df[[\n",
        "        'provider', 'model', 'consistency_normalized', 'performance_normalized'\n",
        "    ]].to_string(index=False))\n",
        "\n",
        "    print(\"\\nNote: Consistency (0-100) and Performance (0-100) normalized scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export--visualization"
      },
      "source": [
        "## 6. Export & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-csv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Export results to CSV\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "consistency_path = os.path.join(\n",
        "    selected_config.output_dir,\n",
        "    f\"consistency_results_{timestamp}.csv\"\n",
        ")\n",
        "consistency_df.to_csv(consistency_path, index=False)\n",
        "print(f\"Consistency results exported to: {consistency_path}\")\n",
        "\n",
        "performance_path = os.path.join(\n",
        "    selected_config.output_dir,\n",
        "    f\"performance_results_{timestamp}.csv\"\n",
        ")\n",
        "performance_df.to_csv(performance_path, index=False)\n",
        "print(f\"Performance results exported to: {performance_path}\")\n",
        "\n",
        "if 'comparison_df' in locals():\n",
        "    comparison_path = os.path.join(\n",
        "        selected_config.output_dir,\n",
        "        f\"comparison_results_{timestamp}.csv\"\n",
        "    )\n",
        "    comparison_df.to_csv(comparison_path, index=False)\n",
        "    print(f\"Comparison results exported to: {comparison_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-json",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Export results to JSON with metadata\n",
        "\n",
        "export_data = {\n",
        "    'metadata': {\n",
        "        'framework': 'CERT',\n",
        "        'version': '1.0',\n",
        "        'timestamp': results['start_time'],\n",
        "        'duration_seconds': results['duration_seconds'],\n",
        "        'configuration': {\n",
        "            'consistency_trials': selected_config.consistency_trials,\n",
        "            'performance_trials': selected_config.performance_trials,\n",
        "            'embedding_model': selected_config.embedding_model_name\n",
        "        }\n",
        "    },\n",
        "    'results': {\n",
        "        'consistency': [asdict(r) for r in results['consistency']],\n",
        "        'performance': [asdict(r) for r in results['performance']]\n",
        "    }\n",
        "}\n",
        "\n",
        "json_path = os.path.join(\n",
        "    selected_config.output_dir,\n",
        "    f\"benchmark_results_{timestamp}.json\"\n",
        ")\n",
        "\n",
        "with open(json_path, 'w') as f:\n",
        "    json.dump(export_data, f, indent=2)\n",
        "\n",
        "print(f\"Full results exported to: {json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization-consistency",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualization: Consistency comparison\n",
        "\n",
        "if len(consistency_df) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Prepare data\n",
        "    consistency_df_sorted = consistency_df.sort_values('consistency_score', ascending=False)\n",
        "    x_labels = [f\"{row['provider']}/{row['model'].split('-')[-1]}\"\n",
        "                 for _, row in consistency_df_sorted.iterrows()]\n",
        "    y_values = consistency_df_sorted['consistency_score'].values\n",
        "\n",
        "    # Create bar chart\n",
        "    colors = plt.cm.Blues(np.linspace(0.4, 0.8, len(y_values)))\n",
        "    bars = ax.bar(range(len(y_values)), y_values, color=colors, edgecolor='black', linewidth=1.5)\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, y_values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{value:.3f}',\n",
        "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax.set_xticks(range(len(x_labels)))\n",
        "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Consistency Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('CERT Framework: Behavioral Consistency Comparison',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_ylim([0, 1.0])\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    ax.axhline(y=0.85, color='green', linestyle='--', alpha=0.5, label='Excellent (>0.85)')\n",
        "    ax.axhline(y=0.70, color='orange', linestyle='--', alpha=0.5, label='Fair (>0.70)')\n",
        "    ax.legend(loc='lower right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    viz_path = os.path.join(\n",
        "        selected_config.output_dir,\n",
        "        f\"consistency_comparison_{timestamp}.png\"\n",
        "    )\n",
        "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Visualization saved to: {viz_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization-performance",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualization: Performance comparison\n",
        "\n",
        "if len(performance_df) > 0:\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "    # Prepare data\n",
        "    performance_df_sorted = performance_df.sort_values('mean_score', ascending=False)\n",
        "    x_labels = [f\"{row['provider']}/{row['model'].split('-')[-1]}\"\n",
        "                 for _, row in performance_df_sorted.iterrows()]\n",
        "    y_values = performance_df_sorted['mean_score'].values\n",
        "    y_errors = performance_df_sorted['std_score'].values\n",
        "\n",
        "    # Create bar chart with error bars\n",
        "    colors = plt.cm.Oranges(np.linspace(0.4, 0.8, len(y_values)))\n",
        "    bars = ax.bar(range(len(y_values)), y_values, yerr=y_errors,\n",
        "                   color=colors, edgecolor='black', linewidth=1.5,\n",
        "                   capsize=5, error_kw={'linewidth': 2})\n",
        "\n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, y_values):\n",
        "        height = bar.get_height()\n",
        "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
        "               f'{value:.3f}',\n",
        "               ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "    ax.set_xticks(range(len(x_labels)))\n",
        "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "    ax.set_ylabel('Performance Score', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('CERT Framework: Output Quality & Performance Comparison',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_ylim([0, 1.0])\n",
        "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    viz_path = os.path.join(\n",
        "        selected_config.output_dir,\n",
        "        f\"performance_comparison_{timestamp}.png\"\n",
        "    )\n",
        "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Visualization saved to: {viz_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualization-combined",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Visualization: Scatter plot (Consistency vs Performance)\n",
        "\n",
        "if 'comparison_df' in locals() and len(comparison_df) > 1:\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Create scatter plot\n",
        "    colors_map = {'anthropic': '#1f77b4', 'openai': '#ff7f0e',\n",
        "                   'google': '#2ca02c', 'xai': '#d62728'}\n",
        "\n",
        "    for provider in comparison_df['provider'].unique():\n",
        "        provider_data = comparison_df[comparison_df['provider'] == provider]\n",
        "        ax.scatter(provider_data['consistency_normalized'],\n",
        "                  provider_data['performance_normalized'],\n",
        "                  s=300, alpha=0.7, label=provider.upper(),\n",
        "                  color=colors_map.get(provider, 'gray'),\n",
        "                  edgecolor='black', linewidth=1.5)\n",
        "\n",
        "        # Add model labels\n",
        "        for _, row in provider_data.iterrows():\n",
        "            ax.annotate(row['model'].split('-')[-1],\n",
        "                        (row['consistency_normalized'], row['performance_normalized']),\n",
        "                        xytext=(5, 5), textcoords='offset points',\n",
        "                        fontsize=9, fontweight='bold')\n",
        "\n",
        "    ax.set_xlabel('Consistency Score (0-100)', fontsize=12, fontweight='bold')\n",
        "    ax.set_ylabel('Performance Score (0-100)', fontsize=12, fontweight='bold')\n",
        "    ax.set_title('CERT Framework: Consistency vs Performance Trade-off',\n",
        "                 fontsize=14, fontweight='bold', pad=20)\n",
        "    ax.set_xlim([0, 105])\n",
        "    ax.set_ylim([0, 105])\n",
        "    ax.grid(True, alpha=0.3, linestyle='--')\n",
        "    ax.legend(loc='best', fontsize=11)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    viz_path = os.path.join(\n",
        "        selected_config.output_dir,\n",
        "        f\"consistency_vs_performance_{timestamp}.png\"\n",
        "    )\n",
        "    plt.savefig(viz_path, dpi=300, bbox_inches='tight')\n",
        "    print(f\"Scatter plot saved to: {viz_path}\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-results",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create summary report\n",
        "\n",
        "report_content = f\"\"\"CERT FRAMEWORK BENCHMARK REPORT\n",
        "{'='*80}\n",
        "\n",
        "Execution Details:\n",
        "- Timestamp: {results['start_time']}\n",
        "- Duration: {results['duration_seconds']:.1f} seconds\n",
        "- Framework: CERT (Consistency, Effect, Resilience, Trustworthiness)\n",
        "\n",
        "Configuration:\n",
        "- Consistency Trials: {selected_config.consistency_trials}\n",
        "- Performance Trials: {selected_config.performance_trials}\n",
        "- Embedding Model: {selected_config.embedding_model_name}\n",
        "\n",
        "Providers Tested:\n",
        "{json.dumps(selected_config.providers, indent=2)}\n",
        "\n",
        "CONSISTENCY RESULTS\n",
        "{'-'*80}\n",
        "{consistency_df[['provider', 'model', 'consistency_score', 'num_trials']].to_string(index=False)}\n",
        "\n",
        "PERFORMANCE RESULTS\n",
        "{'-'*80}\n",
        "{performance_df[['provider', 'model', 'mean_score', 'std_score', 'num_trials']].to_string(index=False)}\n",
        "\n",
        "Results exported to: {selected_config.output_dir}\n",
        "\n",
        "Files generated:\n",
        "- consistency_results_{timestamp}.csv\n",
        "- performance_results_{timestamp}.csv\n",
        "- comparison_results_{timestamp}.csv\n",
        "- benchmark_results_{timestamp}.json\n",
        "- consistency_comparison_{timestamp}.png\n",
        "- performance_comparison_{timestamp}.png\n",
        "- consistency_vs_performance_{timestamp}.png\n",
        "\"\"\"\n",
        "\n",
        "report_path = os.path.join(\n",
        "    selected_config.output_dir,\n",
        "    f\"benchmark_report_{timestamp}.txt\"\n",
        ")\n",
        "\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(report_content)\n",
        "\n",
        "print(\"BENCHMARK SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(report_content)\n",
        "print(f\"\\nReport saved to: {report_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "display-files",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# List all generated files\n",
        "\n",
        "import os\n",
        "\n",
        "print(f\"\\nGenerated files in {selected_config.output_dir}:\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "files = os.listdir(selected_config.output_dir)\n",
        "for file in sorted(files):\n",
        "    file_path = os.path.join(selected_config.output_dir, file)\n",
        "    file_size = os.path.getsize(file_path)\n",
        "    print(f\"  {file:<50} ({file_size:,} bytes)\")\n",
        "\n",
        "print(f\"\\nTotal files: {len(files)}\")\n",
        "print(f\"\\nTo download files in Colab, use:\")\n",
        "print(f\"  from google.colab import files\")\n",
        "print(f\"  files.download('{report_path}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "additional-notes"
      },
      "source": [
        "## Additional Notes\n",
        "\n",
        "### Framework Description\n",
        "\n",
        "**CERT Framework** measures AI model reliability across four dimensions:\n",
        "\n",
        "1. **Consistency (C)**: Behavioral reliability across multiple trials using semantic embeddings\n",
        "   - Measures: How predictable is the model's behavior?\n",
        "   - Score: 0.0 (completely inconsistent) to 1.0 (perfectly consistent)\n",
        "   - Use case: Compliance, audit, regulated workflows\n",
        "\n",
        "2. **Effect (E)**: Multi-agent coordination performance\n",
        "   - Measures: How well do models work together in larger systems?\n",
        "   - Not implemented in this basic version\n",
        "\n",
        "3. **Resilience (R)**: Performance under stress and edge cases\n",
        "   - Not implemented in this basic version\n",
        "\n",
        "4. **Trustworthiness (T)**: Reliability and explicability\n",
        "   - Not implemented in this basic version\n",
        "\n",
        "### Extending the Framework\n",
        "\n",
        "To add more tests:\n",
        "\n",
        "1. Add new test method to `CERTBenchmarkEngine` class\n",
        "2. Create corresponding result dataclass\n",
        "3. Call from `run_full_benchmark` method\n",
        "4. Export results to CSV/JSON\n",
        "\n",
        "### Customization Examples\n",
        "\n",
        "**Add custom prompts:**\n",
        "```python\n",
        "config.consistency_prompt = \"Your custom prompt here\"\n",
        "config.performance_prompts = [\"Prompt 1\", \"Prompt 2\", ...]\n",
        "```\n",
        "\n",
        "**Modify scoring logic:**\n",
        "Edit the `_score_response` method in `CERTBenchmarkEngine` class\n",
        "\n",
        "**Add new provider:**\n",
        "Create new class inheriting from `ProviderInterface` and implement `call_model` method\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "**API Key issues:**\n",
        "- Store API keys in Google Colab Secrets (Keys icon)\n",
        "- Ensure key names match: `ANTHROPIC_API_KEY`, `OPENAI_API_KEY`, `GOOGLE_API_KEY`, `XAI_API_KEY`\n",
        "\n",
        "**Memory issues:**\n",
        "- Reduce trial counts in configuration\n",
        "- Use smaller embedding model\n",
        "- Run one provider at a time\n",
        "\n",
        "**Rate limiting:**\n",
        "- Benchmark respects provider rate limits\n",
        "- Adjust delays between requests as needed\n",
        "- Start with fewer trials\n",
        "\n",
        "### Reproducibility\n",
        "\n",
        "To ensure reproducibility:\n",
        "- Random seed is set to 42 (configurable)\n",
        "- All results are timestamped\n",
        "- Configuration is saved with results\n",
        "- Embedding model version is recorded\n",
        "- Run timestamp preserves execution order\n",
        "\n",
        "### Citation\n",
        "\n",
        "If using this framework in research, please cite:\n",
        "\n",
        "```\n",
        "CERT Benchmarking Framework (2025)\n",
        "Consistency, Effect, Resilience, Trustworthiness Analysis\n",
        "for Language Model Providers\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}