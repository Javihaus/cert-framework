{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CERT Framework - Quick Start in Google Colab\n",
    "\n",
    "This notebook shows you how to use CERT Framework to test LLM reliability.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/cert-framework/blob/master/examples/colab/quickstart.ipynb)\n",
    "\n",
    "## What is CERT?\n",
    "\n",
    "CERT (Consistency Evaluation and Reliability Testing) helps you:\n",
    "- üéØ **Test Consistency**: Measure how reliably your LLM produces the same output\n",
    "- ‚úÖ **Test Accuracy**: Verify outputs match expected ground truth\n",
    "- üîç **Diagnose Issues**: Get automatic diagnosis and actionable suggestions\n",
    "- üìä **Track Metrics**: Monitor performance over time\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install CERT Framework\n",
    "!pip install cert-framework\n",
    "\n",
    "# Optional: Install with extras\n",
    "# !pip install cert-framework[langchain]  # For LangChain support\n",
    "# !pip install cert-framework[inspector]  # For Web UI\n",
    "# !pip install cert-framework[all]        # Everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Test Consistency of a Simple Function\n",
    "\n",
    "Let's test a function that simulates an LLM with some variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cert import TestRunner, TestConfig, GroundTruth\n",
    "import random\n",
    "\n",
    "# Create a test runner\n",
    "runner = TestRunner()\n",
    "\n",
    "# Define a function to test (simulating an LLM with some variance)\n",
    "async def my_llm_function():\n",
    "    \"\"\"Simulated LLM that returns slightly different answers\"\"\"\n",
    "    responses = [\n",
    "        \"The capital of France is Paris.\",\n",
    "        \"Paris is the capital of France.\",\n",
    "        \"France's capital is Paris.\",\n",
    "    ]\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Add ground truth\n",
    "runner.add_ground_truth(GroundTruth(\n",
    "    id=\"simple-test\",\n",
    "    question=\"What is the capital of France?\",\n",
    "    expected=\"Paris\",\n",
    "    metadata={\"correctPages\": [1]}\n",
    "))\n",
    "\n",
    "# Test retrieval (required for layer enforcement)\n",
    "retrieval_result = await runner.test_retrieval(\n",
    "    \"simple-test\",\n",
    "    lambda q: [{\"pageNum\": 1, \"content\": \"Paris is the capital\"}],\n",
    "    {\"precisionMin\": 0.8}\n",
    ")\n",
    "print(f\"‚úÖ Retrieval: {retrieval_result.status}\")\n",
    "\n",
    "# Test accuracy\n",
    "accuracy_result = await runner.test_accuracy(\n",
    "    \"simple-test\",\n",
    "    my_llm_function,\n",
    "    {\"threshold\": 0.8}\n",
    ")\n",
    "print(f\"‚úÖ Accuracy: {accuracy_result.status} ({accuracy_result.accuracy:.2%})\")\n",
    "\n",
    "# Configure consistency test\n",
    "config = TestConfig(\n",
    "    n_trials=5,\n",
    "    consistency_threshold=0.8,\n",
    "    accuracy_threshold=0.8,\n",
    "    semantic_comparison=True\n",
    ")\n",
    "\n",
    "# Run consistency test\n",
    "result = await runner.test_consistency(\n",
    "    \"simple-test\",\n",
    "    my_llm_function,\n",
    "    config\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Results:\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Consistency: {result.consistency:.2%}\")\n",
    "print(f\"Unique outputs: {result.evidence.unique_count if result.evidence else 'N/A'}\")\n",
    "\n",
    "if result.status == 'fail':\n",
    "    print(f\"\\n‚ùå Diagnosis: {result.diagnosis}\")\n",
    "    print(f\"\\nüí° Suggestions:\")\n",
    "    for suggestion in result.suggestions:\n",
    "        print(f\"  - {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Test a More Realistic LLM\n",
    "\n",
    "Now let's test with a function that has actual variance issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Simulate an LLM with high variance\n",
    "counter = 0\n",
    "\n",
    "async def inconsistent_llm():\n",
    "    \"\"\"LLM that returns different answers each time\"\"\"\n",
    "    global counter\n",
    "    counter += 1\n",
    "    \n",
    "    # Different responses each time\n",
    "    responses = [\n",
    "        \"42\",\n",
    "        \"The answer is 42\",\n",
    "        \"forty-two\",\n",
    "        \"4 + 38 = 42\",\n",
    "        \"6 * 7\"\n",
    "    ]\n",
    "    return responses[counter % len(responses)]\n",
    "\n",
    "# Add ground truth\n",
    "runner.add_ground_truth(GroundTruth(\n",
    "    id=\"variance-test\",\n",
    "    question=\"What is 6 * 7?\",\n",
    "    expected=\"42\",\n",
    "    metadata={\"correctPages\": [1]}\n",
    "))\n",
    "\n",
    "# Test retrieval\n",
    "await runner.test_retrieval(\n",
    "    \"variance-test\",\n",
    "    lambda q: [{\"pageNum\": 1}],\n",
    "    {\"precisionMin\": 0.8}\n",
    ")\n",
    "\n",
    "# Test accuracy\n",
    "await runner.test_accuracy(\n",
    "    \"variance-test\",\n",
    "    inconsistent_llm,\n",
    "    {\"threshold\": 0.8}\n",
    ")\n",
    "\n",
    "# Test consistency (should fail)\n",
    "result = await runner.test_consistency(\n",
    "    \"variance-test\",\n",
    "    inconsistent_llm,\n",
    "    TestConfig(\n",
    "        n_trials=10,\n",
    "        consistency_threshold=0.9,\n",
    "        accuracy_threshold=0.8,\n",
    "        semantic_comparison=True\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Consistency Test:\")\n",
    "print(f\"Status: {result.status}\")\n",
    "print(f\"Consistency: {result.consistency:.2%}\")\n",
    "\n",
    "if result.evidence:\n",
    "    print(f\"\\nüîç Evidence:\")\n",
    "    print(f\"Unique outputs: {result.evidence.unique_count}/{len(result.evidence.outputs)}\")\n",
    "    print(f\"Examples:\")\n",
    "    for i, example in enumerate(result.evidence.examples[:3], 1):\n",
    "        print(f\"  {i}. {example}\")\n",
    "\n",
    "if result.diagnosis:\n",
    "    print(f\"\\n‚ùå Diagnosis: {result.diagnosis}\")\n",
    "\n",
    "if result.suggestions:\n",
    "    print(f\"\\nüí° Suggestions:\")\n",
    "    for suggestion in result.suggestions:\n",
    "        print(f\"  - {suggestion}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using with LangChain (Optional)\n",
    "\n",
    "If you have a LangChain chain, you can wrap it with CERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install cert-framework[langchain] langchain-openai\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from cert.langchain_integration import wrap_chain\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key-here\"\n",
    "\n",
    "# Create a simple chain\n",
    "llm = OpenAI(temperature=0.7)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Answer this question concisely: {question}\"\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Wrap with CERT\n",
    "cert_chain = wrap_chain(chain, \"langchain-test\")\n",
    "cert_chain = cert_chain.with_consistency(threshold=0.9, n_trials=5)\n",
    "\n",
    "# Run the chain with testing\n",
    "try:\n",
    "    result = await cert_chain.ainvoke({\"question\": \"What is 2+2?\"})\n",
    "    print(f\"‚úÖ Result: {result}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Semantic Comparison\n",
    "\n",
    "CERT automatically handles semantically equivalent outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cert import SemanticComparator\n",
    "\n",
    "comparator = SemanticComparator()\n",
    "\n",
    "# Test various equivalent formats\n",
    "test_cases = [\n",
    "    (\"$391 billion\", \"391B\"),\n",
    "    (\"$391 billion\", \"$391,000,000,000\"),\n",
    "    (\"Paris\", \"paris\"),\n",
    "    (\"Paris\", \"Paris, France\"),\n",
    "]\n",
    "\n",
    "print(\"üîç Semantic Comparison Tests:\\n\")\n",
    "for expected, actual in test_cases:\n",
    "    result = comparator.compare(expected, actual)\n",
    "    status = \"‚úÖ\" if result.matched else \"‚ùå\"\n",
    "    print(f\"{status} '{expected}' vs '{actual}'\")\n",
    "    print(f\"   Matched: {result.matched}, Confidence: {result.confidence:.2f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Try with your own LLM**: Replace `my_llm_function` with your actual LLM calls\n",
    "2. **Add more tests**: Test different scenarios and edge cases\n",
    "3. **Track over time**: Use the SQLite storage to monitor degradation\n",
    "4. **Visual inspection**: Install `cert-framework[inspector]` and run `cert inspect`\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [GitHub Repository](https://github.com/Javihaus/cert-framework)\n",
    "- [Documentation](https://github.com/Javihaus/cert-framework#readme)\n",
    "- [Report Issues](https://github.com/Javihaus/cert-framework/issues)\n",
    "\n",
    "## Support\n",
    "\n",
    "If you find CERT useful, please ‚≠ê the repository on GitHub!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
