'use client';

import { useState } from 'react';
import {
  Box,
  Button,
  Flex,
  Grid,
  Text,
  Code,
} from '@chakra-ui/react';
import {
  Tabs,
  TabList,
  Tab,
  TabPanels,
  TabPanel,
} from '@chakra-ui/tabs';
import { MdCheckCircle, MdCancel, MdAssessment, MdList } from 'react-icons/md';
import Header from '@/components/Header';
import FileUpload from '@/components/FileUpload';
import MetricCard from '@/components/MetricCard';
import Card from '@/components/Card';
import FailedTracesView from '@/components/FailedTracesView';
import { EvaluationSummary, EvaluationResult } from '@/types/cert';

export default function Home() {
  const [evaluationData, setEvaluationData] = useState<{
    summary: EvaluationSummary;
    results: EvaluationResult[];
  } | null>(null);

  const handleEvaluationFileLoad = (data: any) => {
    console.log('Loaded data:', data);

    if (!data || typeof data !== 'object') {
      alert('Invalid file: Expected a JSON object');
      return;
    }

    if (!data.summary) {
      alert('Invalid file: Missing "summary" field. Please upload a file generated by cert.evaluation.Evaluator');
      return;
    }

    // Handle different file formats
    let results = data.results;

    // If no results array, try to build from failed_examples and high_scoring_examples
    if (!results || !Array.isArray(results)) {
      const failedExamples = data.failed_examples || [];
      const highScoringExamples = data.high_scoring_examples || [];

      // Convert examples to results format
      results = [
        ...failedExamples.map((ex: any) => ({
          timestamp: ex.timestamp,
          query: ex.input,
          response: ex.output,
          measurement: {
            confidence: ex.score,
            rule: 'evaluation',
            components_used: ['semantic', 'grounding']
          },
          passed: false,
          duration_ms: 0
        })),
        ...highScoringExamples.map((ex: any) => ({
          timestamp: ex.timestamp,
          query: ex.input,
          response: ex.output,
          measurement: {
            confidence: ex.score,
            rule: 'evaluation',
            components_used: ['semantic', 'grounding']
          },
          passed: true,
          duration_ms: 0
        }))
      ];

      // If still no results, use empty array (summary-only mode)
      if (results.length === 0) {
        results = [];
      }
    }

    // Normalize summary field names for different formats
    const normalizedSummary = {
      total_traces: data.summary.total_traces,
      evaluated_traces: data.summary.evaluated_traces || data.summary.total_traces,
      passed_traces: data.summary.passed_traces || data.summary.passed,
      failed_traces: data.summary.failed_traces || data.summary.failed,
      accuracy: data.summary.accuracy || data.summary.pass_rate,
      mean_confidence: data.summary.mean_confidence || data.summary.mean_score,
      threshold_used: data.summary.threshold_used || data.threshold_analysis?.current_threshold || 0.7,
      date_range: data.summary.date_range || {
        start: data.temporal_analysis?.period_start || 'N/A',
        end: data.temporal_analysis?.period_end || 'N/A'
      }
    };

    setEvaluationData({ summary: normalizedSummary, results });
  };

  if (!evaluationData) {
    return (
      <Box minH="100vh" bg="secondaryGray.300">
        <Header />
        <Box maxW="1200px" mx="auto" px="20px" py="30px">
          <FileUpload
            onFileLoad={handleEvaluationFileLoad}
            accept=".json"
            label="Upload Evaluation Results"
          />

          <Card mt="20px">
            <Text fontSize="md" fontWeight="700" color="secondaryGray.900" mb="12px">
              How to generate evaluation results:
            </Text>
            <Code
              display="block"
              whiteSpace="pre"
              p="16px"
              borderRadius="12px"
              fontSize="sm"
              bg="secondaryGray.400"
              color="navy.900"
            >
{`from cert.evaluation import Evaluator

evaluator = Evaluator(threshold=0.7)
results = evaluator.evaluate_log_file(
    log_file="production_traces.jsonl",
    output="evaluation_results.json"
)`}
            </Code>
          </Card>
        </Box>
      </Box>
    );
  }

  const { summary, results } = evaluationData;
  const isCompliant = summary.accuracy >= 0.9;

  return (
    <Box minH="100vh" bg="secondaryGray.300">
      <Header />

      <Box maxW="1600px" mx="auto" px="20px" py="30px">
        <Button
          onClick={() => setEvaluationData(null)}
          mb="20px"
          bg="white"
          color="secondaryGray.900"
          _hover={{ bg: 'secondaryGray.100' }}
        >
          ‚Üê Load Different File
        </Button>

        {/* Status Banner */}
        <Box
          bg={isCompliant ? 'green.500' : 'orange.400'}
          color="white"
          p="20px"
          borderRadius="12px"
          mb="24px"
        >
          <Flex justify="space-between" align="center">
            <Box>
              <Text fontSize="18px" fontWeight="600" mb="4px">
                {isCompliant ? '‚úÖ Compliant' : '‚ö†Ô∏è Below Compliance Threshold'}
              </Text>
              <Text fontSize="14px">
                Accuracy at {(summary.accuracy * 100).toFixed(1)}% (target: 90%)
                {!isCompliant && ` - Review ${summary.failed_traces} failed traces.`}
              </Text>
            </Box>
          </Flex>
        </Box>

        {/* Metrics Grid */}
        <Grid
          templateColumns={{
            base: '1fr',
            md: 'repeat(2, 1fr)',
            lg: 'repeat(4, 1fr)',
          }}
          gap="20px"
          mb="20px"
        >
          <MetricCard
            label="Accuracy"
            value={`${(summary.accuracy * 100).toFixed(1)}%`}
            icon={MdAssessment}
            color={summary.accuracy >= 0.9 ? 'green' : summary.accuracy >= 0.8 ? 'orange' : 'red'}
          />
          <MetricCard
            label="Total Traces"
            value={summary.total_traces.toString()}
            icon={MdList}
            color="blue"
          />
          <MetricCard
            label="Passed"
            value={summary.passed_traces.toString()}
            icon={MdCheckCircle}
            color="green"
          />
          <MetricCard
            label="Failed"
            value={summary.failed_traces.toString()}
            icon={MdCancel}
            color="red"
          />
        </Grid>

        {/* Tabs */}
        <Tabs colorScheme="brand" variant="enclosed">
          <TabList bg="white" borderRadius="12px 12px 0 0">
            <Tab>Overview</Tab>
            <Tab>Failed Traces ({summary.failed_traces})</Tab>
            <Tab>Distribution</Tab>
            <Tab>Documentation</Tab>
          </TabList>

          <TabPanels bg="white" borderRadius="0 0 12px 12px" p="20px">
            {/* Overview Tab */}
            <TabPanel>
              <Grid
                templateColumns={{ base: '1fr', lg: 'repeat(2, 1fr)' }}
                gap="20px"
              >
                <Card>
                  <Text fontSize="md" fontWeight="700" color="secondaryGray.600" mb="4px">
                    Mean Confidence
                  </Text>
                  <Text fontSize="48px" fontWeight="700" color="brand.500">
                    {summary.mean_confidence.toFixed(3)}
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.600" mt="8px">
                    Threshold: {summary.threshold_used.toFixed(2)}
                  </Text>
                </Card>

                <Card>
                  <Text fontSize="md" fontWeight="700" color="secondaryGray.600" mb="12px">
                    Evaluation Period
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700" mb="4px">
                    <strong>Start:</strong> {summary.date_range.start}
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700">
                    <strong>End:</strong> {summary.date_range.end}
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.600" mt="12px">
                    Total traces evaluated: {summary.evaluated_traces.toLocaleString()}
                  </Text>
                </Card>

                <Card gridColumn={{ base: '1', lg: 'span 2' }}>
                  <Text fontSize="md" fontWeight="700" color="secondaryGray.900" mb="12px">
                    Compliance Status
                  </Text>
                  <Box
                    bg={isCompliant ? 'green.50' : 'orange.50'}
                    border="2px solid"
                    borderColor={isCompliant ? 'green.200' : 'orange.200'}
                    p="16px"
                    borderRadius="8px"
                  >
                    <Text fontSize="lg" fontWeight="600" color={isCompliant ? 'green.700' : 'orange.700'}>
                      {isCompliant
                        ? '‚úÖ System meets EU AI Act Article 15 requirements'
                        : '‚ö†Ô∏è Action Required: System below 90% accuracy threshold'
                      }
                    </Text>
                    {!isCompliant && (
                      <Text fontSize="sm" color="orange.600" mt="8px">
                        Review failed traces and consider adjusting threshold or improving system accuracy.
                      </Text>
                    )}
                  </Box>
                </Card>
              </Grid>
            </TabPanel>

            {/* Failed Traces Tab */}
            <TabPanel>
              {results.length > 0 ? (
                <FailedTracesView results={results} threshold={summary.threshold_used} />
              ) : (
                <Card>
                  <Text fontSize="md" fontWeight="700" color="secondaryGray.900" mb="8px">
                    No Detailed Trace Data
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.600">
                    This file contains summary metrics only. {summary.failed_traces} traces failed based on summary data.
                  </Text>
                </Card>
              )}
            </TabPanel>

            {/* Distribution Tab */}
            <TabPanel>
              <Card>
                <Text fontSize="lg" fontWeight="700" color="secondaryGray.900" mb="12px">
                  Score Distribution
                </Text>
                <Text fontSize="sm" color="secondaryGray.600">
                  Distribution visualization coming soon...
                </Text>
                <Box mt="20px" p="20px" bg="gray.50" borderRadius="8px">
                  <Text fontSize="xs" color="gray.500">
                    This tab will show:
                  </Text>
                  <Text fontSize="xs" color="gray.500" mt="4px">
                    ‚Ä¢ Histogram of confidence scores
                  </Text>
                  <Text fontSize="xs" color="gray.500">
                    ‚Ä¢ Threshold line visualization
                  </Text>
                  <Text fontSize="xs" color="gray.500">
                    ‚Ä¢ Pass/fail distribution
                  </Text>
                </Box>
              </Card>
            </TabPanel>

            {/* Documentation Tab */}
            <TabPanel>
              <Card>
                <Text fontSize="lg" fontWeight="700" color="secondaryGray.900" mb="16px">
                  Understanding CERT Metrics
                </Text>

                <Box mb="20px">
                  <Text fontSize="md" fontWeight="600" color="secondaryGray.900" mb="8px">
                    Accuracy
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700">
                    The percentage of traces that passed the confidence threshold. EU AI Act Article 15
                    requires "appropriate levels of accuracy" - typically 90% or higher for high-risk systems.
                  </Text>
                </Box>

                <Box mb="20px">
                  <Text fontSize="md" fontWeight="600" color="secondaryGray.900" mb="8px">
                    Confidence Score
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700">
                    A value between 0.0 and 1.0 measuring the reliability of an AI output. Scores are
                    computed using semantic similarity and term grounding components.
                  </Text>
                </Box>

                <Box mb="20px">
                  <Text fontSize="md" fontWeight="600" color="secondaryGray.900" mb="8px">
                    Threshold
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700">
                    The minimum confidence score required for a trace to pass. Default is 0.70, but can
                    be calibrated based on your domain (financial: 0.46, healthcare: 0.50, legal: 0.48).
                  </Text>
                </Box>

                <Box mb="20px">
                  <Text fontSize="md" fontWeight="600" color="secondaryGray.900" mb="8px">
                    Failure Patterns
                  </Text>
                  <Text fontSize="sm" color="secondaryGray.700" mb="4px">
                    Failed traces are automatically classified into patterns:
                  </Text>
                  <Box pl="12px" mt="8px">
                    <Text fontSize="sm" color="red.600">‚Ä¢ <strong>Irrelevant:</strong> Completely off-topic responses</Text>
                    <Text fontSize="sm" color="orange.600">‚Ä¢ <strong>Incomplete:</strong> Vague or missing details</Text>
                    <Text fontSize="sm" color="yellow.600">‚Ä¢ <strong>Missing Info:</strong> No specific data provided</Text>
                    <Text fontSize="sm" color="blue.600">‚Ä¢ <strong>Definition Only:</strong> Defines without explaining</Text>
                  </Box>
                </Box>

                <Box bg="blue.50" p="16px" borderRadius="8px">
                  <Text fontSize="sm" fontWeight="600" color="blue.700" mb="4px">
                    üí° Article 19 Compliance
                  </Text>
                  <Text fontSize="xs" color="blue.600">
                    All traces shown in this dashboard are automatically logged per EU AI Act Article 19
                    requirements, providing complete audit trail for regulatory review.
                  </Text>
                </Box>
              </Card>
            </TabPanel>
          </TabPanels>
        </Tabs>
      </Box>
    </Box>
  );
}
