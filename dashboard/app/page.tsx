'use client';

import { useState } from 'react';
import {
  Box,
  Button,
  Grid,
  Text,
  Code,
} from '@chakra-ui/react';
import { MdCheckCircle, MdCancel, MdAssessment, MdList } from 'react-icons/md';
import Navigation from '@/components/Navigation';
import FileUpload from '@/components/FileUpload';
import MetricCard from '@/components/MetricCard';
import Card from '@/components/Card';
import StatusBanner from '@/components/StatusBanner';
import QuickActions from '@/components/QuickActions';
import FailedTracesView from '@/components/FailedTracesView';
import DistributionChart from '@/components/DistributionChart';
import DocumentationContent from '@/components/DocumentationContent';
import { EvaluationSummary, EvaluationResult } from '@/types/cert';
import { colors } from '@/theme/colors';

export default function Home() {
  const [evaluationData, setEvaluationData] = useState<{
    summary: EvaluationSummary;
    results: EvaluationResult[];
  } | null>(null);
  const [activeTab, setActiveTab] = useState('load');

  const handleEvaluationFileLoad = (data: any) => {
    console.log('Loaded data:', data);

    if (!data || typeof data !== 'object') {
      alert('Invalid file: Expected a JSON object');
      return;
    }

    if (!data.summary) {
      alert('Invalid file: Missing "summary" field. Please upload a file generated by cert.evaluation.Evaluator');
      return;
    }

    // Handle different file formats
    let results = data.results;

    // If no results array, try to build from failed_examples and high_scoring_examples
    if (!results || !Array.isArray(results)) {
      const failedExamples = data.failed_examples || [];
      const highScoringExamples = data.high_scoring_examples || [];

      // Convert examples to results format
      results = [
        ...failedExamples.map((ex: any) => ({
          timestamp: ex.timestamp,
          query: ex.input,
          response: ex.output,
          measurement: {
            confidence: ex.score,
            rule: 'evaluation',
            components_used: ['semantic', 'grounding']
          },
          passed: false,
          duration_ms: 0
        })),
        ...highScoringExamples.map((ex: any) => ({
          timestamp: ex.timestamp,
          query: ex.input,
          response: ex.output,
          measurement: {
            confidence: ex.score,
            rule: 'evaluation',
            components_used: ['semantic', 'grounding']
          },
          passed: true,
          duration_ms: 0
        }))
      ];

      // If still no results, use empty array (summary-only mode)
      if (results.length === 0) {
        results = [];
      }
    }

    // Normalize summary field names for different formats
    const normalizedSummary = {
      total_traces: data.summary.total_traces,
      evaluated_traces: data.summary.evaluated_traces || data.summary.total_traces,
      passed_traces: data.summary.passed_traces || data.summary.passed,
      failed_traces: data.summary.failed_traces || data.summary.failed,
      accuracy: data.summary.accuracy || data.summary.pass_rate,
      mean_confidence: data.summary.mean_confidence || data.summary.mean_score,
      threshold_used: data.summary.threshold_used || data.threshold_analysis?.current_threshold || 0.7,
      date_range: data.summary.date_range || {
        start: data.temporal_analysis?.period_start || 'N/A',
        end: data.temporal_analysis?.period_end || 'N/A'
      }
    };

    setEvaluationData({ summary: normalizedSummary, results });
    // Switch to overview tab after loading data
    setActiveTab('overview');
  };

  const handleExportCSV = () => {
    if (!evaluationData) return;

    const { results } = evaluationData;
    const headers = ['Timestamp', 'Query', 'Response', 'Confidence', 'Passed'];
    const rows = results.map(r => [
      r.timestamp,
      `"${r.query.replace(/"/g, '""')}"`,
      `"${(r.response || '').replace(/"/g, '""')}"`,
      r.measurement.confidence.toFixed(3),
      r.passed ? 'Yes' : 'No',
    ]);

    const csv = [headers.join(','), ...rows.map(row => row.join(','))].join('\n');
    const blob = new Blob([csv], { type: 'text/csv' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `evaluation_results_${new Date().toISOString().split('T')[0]}.csv`;
    a.click();
    URL.revokeObjectURL(url);
  };

  const summary = evaluationData?.summary;
  const results = evaluationData?.results || [];
  const isCompliant = summary ? summary.accuracy >= 0.9 : false;

  // Render active tab content
  const renderTabContent = () => {
    switch (activeTab) {
      case 'load':
        return (
          <Box maxW="1200px" mx="auto">
            <Box mb="40px" textAlign="center">
              <Text
                fontSize="28px"
                fontWeight="700"
                color={colors.navy}
                mb="8px"
                letterSpacing="-0.5px"
              >
                Upload Evaluation Results
              </Text>
              <Text fontSize="15px" color={colors.text.muted}>
                Load your CERT evaluation data to view compliance metrics and analysis
              </Text>
            </Box>

            <FileUpload
              onFileLoad={handleEvaluationFileLoad}
              accept=".json"
              label="Upload Evaluation Results"
            />

            <Box maxW="60%" mx="auto" mt="32px">
              <Card style={{ borderColor: colors.patience, background: 'white' }}>
                <Text fontSize="16px" fontWeight="700" color={colors.navy} mb="16px">
                  How to generate evaluation results:
                </Text>
                <Code
                  display="block"
                  whiteSpace="pre"
                  p="20px"
                  borderRadius="12px"
                  fontSize="14px"
                  bg={colors.patience}
                  color={colors.navy}
                  lineHeight="1.6"
                >
{`from cert.evaluation import Evaluator

evaluator = Evaluator(threshold=0.7)
results = evaluator.evaluate_log_file(
    log_file="production_traces.jsonl",
    output="evaluation_results.json"
)`}
                </Code>
              </Card>
            </Box>
          </Box>
        );

      case 'overview':
        if (!evaluationData) return null;
        return (
          <>
            <StatusBanner
              isCompliant={isCompliant}
              accuracy={summary!.accuracy}
              failedCount={summary!.failed_traces}
            />

            {/* Metrics Grid */}
            <Grid
              templateColumns={{
                base: '1fr',
                md: 'repeat(2, 1fr)',
                lg: 'repeat(4, 1fr)',
              }}
              gap="20px"
              mb="20px"
            >
              <MetricCard
                label="Accuracy"
                value={`${(summary!.accuracy * 100).toFixed(1)}%`}
                icon={MdAssessment}
                color={summary!.accuracy >= 0.9 ? 'green' : summary!.accuracy >= 0.8 ? 'orange' : 'red'}
              />
              <MetricCard
                label="Total Traces"
                value={summary!.total_traces.toString()}
                icon={MdList}
                color="blue"
              />
              <MetricCard
                label="Passed"
                value={summary!.passed_traces.toString()}
                icon={MdCheckCircle}
                color="green"
              />
              <MetricCard
                label="Failed"
                value={summary!.failed_traces.toString()}
                icon={MdCancel}
                color="red"
              />
            </Grid>

            <Grid templateColumns={{ base: '1fr', lg: 'repeat(2, 1fr)' }} gap="20px" mb="20px">
              <Card style={{ borderColor: colors.patience }}>
                <Text fontSize="md" fontWeight="700" color={colors.text.muted} mb="4px">
                  Mean Confidence
                </Text>
                <Text fontSize="48px" fontWeight="700" color={colors.cobalt}>
                  {summary!.mean_confidence.toFixed(3)}
                </Text>
                <Text fontSize="sm" color={colors.text.muted} mt="8px">
                  Threshold: {summary!.threshold_used.toFixed(2)}
                </Text>
                <Text fontSize="14px" color={colors.text.muted} mt="16px" lineHeight="1.6">
                  Mean confidence of {summary!.mean_confidence.toFixed(3)} suggests {' '}
                  {summary!.mean_confidence > 0.8
                    ? 'strong performance with most predictions highly confident.'
                    : 'moderate performance near the boundary - small improvements will increase compliance.'}
                </Text>
              </Card>

              <Card style={{ borderColor: colors.patience }}>
                <Text fontSize="md" fontWeight="700" color={colors.text.muted} mb="12px">
                  Evaluation Period
                </Text>
                <Text fontSize="sm" color={colors.text.primary} mb="4px">
                  <strong>Start:</strong> {summary!.date_range.start}
                </Text>
                <Text fontSize="sm" color={colors.text.primary}>
                  <strong>End:</strong> {summary!.date_range.end}
                </Text>
                <Text fontSize="sm" color={colors.text.muted} mt="12px">
                  Total traces evaluated: {summary!.evaluated_traces.toLocaleString()}
                </Text>
              </Card>
            </Grid>

            <QuickActions
              onViewFailed={() => setActiveTab('failed')}
              onViewDistribution={() => setActiveTab('distribution')}
              onExport={handleExportCSV}
            />
          </>
        );

      case 'failed':
        if (!evaluationData) return null;
        return results.length > 0 ? (
          <FailedTracesView results={results} threshold={summary!.threshold_used} />
        ) : (
          <Card style={{ borderColor: colors.patience }}>
            <Text fontSize="md" fontWeight="700" color={colors.navy} mb="8px">
              No Detailed Trace Data
            </Text>
            <Text fontSize="sm" color={colors.text.muted}>
              This file contains summary metrics only. {summary!.failed_traces} traces failed based on summary data.
            </Text>
          </Card>
        );

      case 'distribution':
        if (!evaluationData) return null;
        return (
          <>
            <Card style={{ borderColor: colors.patience, marginBottom: '24px' }}>
              <Text fontSize="18px" fontWeight="700" color={colors.navy} mb="16px">
                Score Distribution
              </Text>
              {results.length > 0 ? (
                <DistributionChart results={results} threshold={summary!.threshold_used} />
              ) : (
                <Text fontSize="14px" color={colors.text.muted}>
                  No detailed trace data available for distribution analysis.
                </Text>
              )}
            </Card>

            {results.length > 0 && (
              <Card style={{ borderColor: colors.warning, background: '#FEF3C7' }}>
                <Text fontSize="16px" fontWeight="700" color={colors.navy} mb="8px">
                  Critical Finding
                </Text>
                <Text fontSize="14px" color={colors.text.primary} lineHeight="1.7">
                  {(() => {
                    const borderlineCount = results.filter(r =>
                      r.measurement.confidence >= 0.5 && r.measurement.confidence < summary!.threshold_used
                    ).length;
                    const borderlinePercent = ((borderlineCount / results.length) * 100).toFixed(1);

                    return (
                      <>
                        <strong>{borderlineCount} traces ({borderlinePercent}%)</strong> scored between 0.5-{summary!.threshold_used.toFixed(1)} - just below or near the threshold.
                        These represent borderline cases where small improvements could push you to 90%+ compliance.
                        Focus engineering effort here for maximum impact.
                      </>
                    );
                  })()}
                </Text>
              </Card>
            )}
          </>
        );

      case 'documentation':
        return (
          <Card style={{ borderColor: colors.patience }}>
            <DocumentationContent />
          </Card>
        );

      default:
        return null;
    }
  };

  return (
    <Box minH="100vh" bg={colors.background}>
      <Navigation
        activeTab={activeTab}
        onTabChange={setActiveTab}
        hasData={!!evaluationData}
      />

      <Box maxW="1600px" mx="auto" px="32px" py="32px">
        {renderTabContent()}
      </Box>
    </Box>
  );
}
