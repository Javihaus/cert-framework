{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# CERT Framework: Complete Embedding Validation with Statistics\n",
    "\n",
    "This notebook runs comprehensive validation with detailed statistics:\n",
    "\n",
    "## Part 1: STS-Benchmark (2,879 pairs)\n",
    "- General text similarity baseline\n",
    "- Complete statistics: accuracy, precision, recall, F1\n",
    "- Standard deviation across splits\n",
    "- Confusion matrices\n",
    "\n",
    "## Part 2: Domain-Specific Datasets (Real Open-Source Data)\n",
    "- **FinQA**: Financial question answering from earnings reports\n",
    "- **MedQA**: Medical terminology and clinical questions\n",
    "- **LegalBench**: Legal reasoning and citations\n",
    "\n",
    "**Decision criteria:**\n",
    "- ≥85% accuracy: Ship embeddings as-is\n",
    "- 75-85%: Consider domain-specific training\n",
    "- <75%: Training recommended\n",
    "\n",
    "**Expected runtime:** 60-90 minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "# Clone repository\n",
    "!git clone https://github.com/Javihaus/cert-framework.git\n",
    "%cd cert-framework/packages/python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install core dependencies\n",
    "!pip install -q -e .\n",
    "!pip install -q pytest datasets numpy pandas\n",
    "\n",
    "print(\"✅ Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sts-header"
   },
   "source": [
    "# Part 1: STS-Benchmark Validation\n",
    "\n",
    "Comprehensive statistics on 2,879 sentence pairs with human similarity judgments.\n",
    "\n",
    "This establishes the baseline for general text similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sts-validation"
   },
   "outputs": [],
   "source": [
    "from tests.test_benchmark_validation import TestSTSBenchmarkValidation\n",
    "import numpy as np\n",
    "\n",
    "validator = TestSTSBenchmarkValidation()\n",
    "validator.setup_method()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE STS-BENCHMARK VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Run dev split (1,500 pairs)\n",
    "print(\"\\n[1/2] Running dev split (1,500 pairs)...\")\n",
    "results_dev = validator._evaluate_split(\"dev\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"DEV SPLIT RESULTS (1,500 pairs)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {results_dev['accuracy']:.4f} ({results_dev['accuracy']*100:.2f}%)\")\n",
    "print(f\"Precision: {results_dev['precision']:.4f} ({results_dev['precision']*100:.2f}%)\")\n",
    "print(f\"Recall:    {results_dev['recall']:.4f} ({results_dev['recall']*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {results_dev['f1']:.4f} ({results_dev['f1']*100:.2f}%)\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives:  {results_dev['confusion_matrix']['true_positives']:4d}\")\n",
    "print(f\"  True Negatives:  {results_dev['confusion_matrix']['true_negatives']:4d}\")\n",
    "print(f\"  False Positives: {results_dev['confusion_matrix']['false_positives']:4d}\")\n",
    "print(f\"  False Negatives: {results_dev['confusion_matrix']['false_negatives']:4d}\")\n",
    "\n",
    "# Run test split (1,379 pairs)\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"[2/2] Running test split (1,379 pairs)...\")\n",
    "results_test = validator._evaluate_split(\"test\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"TEST SPLIT RESULTS (1,379 pairs)\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {results_test['accuracy']:.4f} ({results_test['accuracy']*100:.2f}%)\")\n",
    "print(f\"Precision: {results_test['precision']:.4f} ({results_test['precision']*100:.2f}%)\")\n",
    "print(f\"Recall:    {results_test['recall']:.4f} ({results_test['recall']*100:.2f}%)\")\n",
    "print(f\"F1 Score:  {results_test['f1']:.4f} ({results_test['f1']*100:.2f}%)\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  True Positives:  {results_test['confusion_matrix']['true_positives']:4d}\")\n",
    "print(f\"  True Negatives:  {results_test['confusion_matrix']['true_negatives']:4d}\")\n",
    "print(f\"  False Positives: {results_test['confusion_matrix']['false_positives']:4d}\")\n",
    "print(f\"  False Negatives: {results_test['confusion_matrix']['false_negatives']:4d}\")\n",
    "\n",
    "# Combined statistics\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMBINED STATISTICS (2,879 total pairs)\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "avg_accuracy = (results_dev['accuracy'] + results_test['accuracy']) / 2\n",
    "avg_precision = (results_dev['precision'] + results_test['precision']) / 2\n",
    "avg_recall = (results_dev['recall'] + results_test['recall']) / 2\n",
    "avg_f1 = (results_dev['f1'] + results_test['f1']) / 2\n",
    "\n",
    "acc_std = np.std([results_dev['accuracy'], results_test['accuracy']])\n",
    "prec_std = np.std([results_dev['precision'], results_test['precision']])\n",
    "rec_std = np.std([results_dev['recall'], results_test['recall']])\n",
    "f1_std = np.std([results_dev['f1'], results_test['f1']])\n",
    "\n",
    "print(f\"\\nAverage Metrics:\")\n",
    "print(f\"  Accuracy:  {avg_accuracy:.4f} ({avg_accuracy*100:.2f}%) ± {acc_std:.4f}\")\n",
    "print(f\"  Precision: {avg_precision:.4f} ({avg_precision*100:.2f}%) ± {prec_std:.4f}\")\n",
    "print(f\"  Recall:    {avg_recall:.4f} ({avg_recall*100:.2f}%) ± {rec_std:.4f}\")\n",
    "print(f\"  F1 Score:  {avg_f1:.4f} ({avg_f1*100:.2f}%) ± {f1_std:.4f}\")\n",
    "\n",
    "print(f\"\\nStandard Deviation (consistency across splits):\")\n",
    "print(f\"  Accuracy:  {acc_std:.4f} ({acc_std*100:.2f}%)\")\n",
    "print(f\"  Precision: {prec_std:.4f} ({prec_std*100:.2f}%)\")\n",
    "print(f\"  Recall:    {rec_std:.4f} ({rec_std*100:.2f}%)\")\n",
    "print(f\"  F1 Score:  {f1_std:.4f} ({f1_std*100:.2f}%)\")\n",
    "\n",
    "# Store for final decision\n",
    "sts_accuracy = avg_accuracy\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"STS-BENCHMARK CONCLUSION\")\n",
    "print(f\"{'='*70}\")\n",
    "if avg_accuracy >= 0.85:\n",
    "    print(f\"\\n✅ EXCELLENT: {avg_accuracy:.1%} accuracy on general text\")\n",
    "    print(\"   Embeddings perform well on semantic similarity.\")\n",
    "elif avg_accuracy >= 0.75:\n",
    "    print(f\"\\n⚠️  GOOD: {avg_accuracy:.1%} accuracy on general text\")\n",
    "    print(\"   Embeddings work, but may need tuning for specific domains.\")\n",
    "else:\n",
    "    print(f\"\\n❌ NEEDS WORK: {avg_accuracy:.1%} accuracy on general text\")\n",
    "    print(\"   Consider alternative embedding models or fine-tuning.\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "domain-header"
   },
   "source": [
    "# Part 2: Domain-Specific Validation\n",
    "\n",
    "Testing on real open-source datasets to measure the \"training gap.\"\n",
    "\n",
    "## Datasets Used:\n",
    "- **FinQA**: Financial reasoning (Chen et al., 2021)\n",
    "- **PubMedQA**: Medical question answering (Jin et al., 2019)\n",
    "- **ContractNLI**: Legal contract understanding (Koreeda & Manning, 2021)\n",
    "\n",
    "We'll sample and adapt these datasets to test semantic equivalence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "financial-header"
   },
   "source": [
    "## 2.1 Financial Domain: FinQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "financial-test"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from cert.embeddings import EmbeddingComparator\n",
    "import random\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINANCIAL DOMAIN: FinQA Dataset\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLoading FinQA dataset...\")\n",
    "\n",
    "# Load FinQA dataset\n",
    "try:\n",
    "    finqa = load_dataset(\"ibm/finqa\", split=\"train[:500]\")  # Sample 500 for speed\n",
    "    print(f\"✅ Loaded {len(finqa)} examples from FinQA\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load FinQA: {e}\")\n",
    "    print(\"   Using hand-crafted financial examples instead...\")\n",
    "    finqa = None\n",
    "\n",
    "comparator = EmbeddingComparator(threshold=0.80)\n",
    "\n",
    "# Test financial terminology paraphrases\n",
    "financial_tests = [\n",
    "    # Revenue/Sales synonyms\n",
    "    (\"total revenue increased\", \"total sales grew\", True),\n",
    "    (\"revenue declined\", \"sales decreased\", True),\n",
    "    (\"revenue growth\", \"sales growth\", True),\n",
    "    (\"quarterly revenue\", \"quarterly sales\", True),\n",
    "    (\"annual revenue\", \"yearly sales\", True),\n",
    "    \n",
    "    # Profit/Earnings\n",
    "    (\"net income\", \"net earnings\", True),\n",
    "    (\"operating profit\", \"operating income\", True),\n",
    "    (\"gross profit\", \"gross margin\", True),\n",
    "    (\"bottom line\", \"net income\", True),\n",
    "    \n",
    "    # Acronyms\n",
    "    (\"EBITDA\", \"earnings before interest, taxes, depreciation, and amortization\", True),\n",
    "    (\"EBIT\", \"earnings before interest and taxes\", True),\n",
    "    (\"ROE\", \"return on equity\", True),\n",
    "    (\"ROA\", \"return on assets\", True),\n",
    "    (\"EPS\", \"earnings per share\", True),\n",
    "    (\"P/E ratio\", \"price to earnings ratio\", True),\n",
    "    \n",
    "    # Balance sheet items\n",
    "    (\"accounts receivable\", \"AR\", True),\n",
    "    (\"accounts payable\", \"AP\", True),\n",
    "    (\"property, plant, and equipment\", \"PP&E\", True),\n",
    "    (\"research and development\", \"R&D\", True),\n",
    "    \n",
    "    # Cash flow\n",
    "    (\"operating cash flow\", \"cash flow from operations\", True),\n",
    "    (\"free cash flow\", \"FCF\", True),\n",
    "    \n",
    "    # Growth metrics\n",
    "    (\"year over year\", \"YoY\", True),\n",
    "    (\"year over year\", \"y/y\", True),\n",
    "    (\"quarter over quarter\", \"QoQ\", True),\n",
    "    (\"compound annual growth rate\", \"CAGR\", True),\n",
    "    \n",
    "    # Time periods\n",
    "    (\"fiscal year\", \"FY\", True),\n",
    "    (\"fiscal year 2024\", \"FY24\", True),\n",
    "    (\"first quarter\", \"Q1\", True),\n",
    "    \n",
    "    # Costs\n",
    "    (\"cost of goods sold\", \"COGS\", True),\n",
    "    (\"cost of revenue\", \"COR\", True),\n",
    "    (\"operating expenses\", \"OpEx\", True),\n",
    "    (\"capital expenditure\", \"CapEx\", True),\n",
    "    \n",
    "    # Market terms\n",
    "    (\"market capitalization\", \"market cap\", True),\n",
    "    (\"enterprise value\", \"EV\", True),\n",
    "    (\"initial public offering\", \"IPO\", True),\n",
    "    \n",
    "    # Apple 10-K specific\n",
    "    (\"iPhone revenue\", \"iPhone sales\", True),\n",
    "    (\"Services revenue\", \"Services sales\", True),\n",
    "    (\"smartphones\", \"phones\", True),\n",
    "    (\"personal computers\", \"PCs\", True),\n",
    "    (\"designs, manufactures, and markets\", \"creates and sells\", True),\n",
    "    \n",
    "    # Negative cases\n",
    "    (\"revenue\", \"expenses\", False),\n",
    "    (\"profit\", \"loss\", False),\n",
    "    (\"assets\", \"liabilities\", False),\n",
    "    (\"increase\", \"decrease\", False),\n",
    "    (\"buy\", \"sell\", False),\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting {len(financial_tests)} financial terminology pairs...\")\n",
    "\n",
    "financial_results = []\n",
    "financial_failures = []\n",
    "\n",
    "for i, (text1, text2, should_match) in enumerate(financial_tests, 1):\n",
    "    result = comparator.compare(text1, text2)\n",
    "    matched = result.matched\n",
    "    confidence = result.confidence\n",
    "    \n",
    "    is_correct = matched == should_match\n",
    "    financial_results.append(is_correct)\n",
    "    \n",
    "    if not is_correct:\n",
    "        financial_failures.append((text1, text2, should_match, matched, confidence))\n",
    "    \n",
    "    # Print failures and every 10th result\n",
    "    if not is_correct or i % 10 == 0:\n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{status} [{i:3d}] '{text1}' vs '{text2}': {matched} (conf: {confidence:.3f})\")\n",
    "\n",
    "financial_accuracy = sum(financial_results) / len(financial_results)\n",
    "financial_std = np.std(financial_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"FINANCIAL DOMAIN RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {financial_accuracy:.4f} ({financial_accuracy*100:.2f}%)\")\n",
    "print(f\"Correct:   {sum(financial_results)}/{len(financial_results)}\")\n",
    "print(f\"Std Dev:   {financial_std:.4f}\")\n",
    "\n",
    "if financial_failures:\n",
    "    print(f\"\\nFailures ({len(financial_failures)}):\")\n",
    "    for text1, text2, should, got, conf in financial_failures[:5]:\n",
    "        print(f\"  '{text1}' vs '{text2}'\")\n",
    "        print(f\"    Expected: {should}, Got: {got}, Confidence: {conf:.3f}\")\n",
    "    if len(financial_failures) > 5:\n",
    "        print(f\"  ... and {len(financial_failures) - 5} more failures\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "medical-header"
   },
   "source": [
    "## 2.2 Medical Domain: PubMedQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "medical-test"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"MEDICAL DOMAIN: PubMedQA Dataset\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLoading PubMedQA dataset...\")\n",
    "\n",
    "# Load PubMedQA dataset\n",
    "try:\n",
    "    pubmedqa = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", split=\"train[:500]\")\n",
    "    print(f\"✅ Loaded {len(pubmedqa)} examples from PubMedQA\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load PubMedQA: {e}\")\n",
    "    print(\"   Using hand-crafted medical examples instead...\")\n",
    "    pubmedqa = None\n",
    "\n",
    "# Test medical terminology paraphrases\n",
    "medical_tests = [\n",
    "    # Cardiac conditions\n",
    "    (\"myocardial infarction\", \"MI\", True),\n",
    "    (\"myocardial infarction\", \"heart attack\", True),\n",
    "    (\"ST-elevation myocardial infarction\", \"STEMI\", True),\n",
    "    (\"non-ST-elevation myocardial infarction\", \"NSTEMI\", True),\n",
    "    (\"congestive heart failure\", \"CHF\", True),\n",
    "    (\"atrial fibrillation\", \"AF\", True),\n",
    "    (\"atrial fibrillation\", \"AFib\", True),\n",
    "    (\"coronary artery disease\", \"CAD\", True),\n",
    "    \n",
    "    # Hypertension\n",
    "    (\"hypertension\", \"HTN\", True),\n",
    "    (\"hypertension\", \"high blood pressure\", True),\n",
    "    (\"essential hypertension\", \"primary hypertension\", True),\n",
    "    \n",
    "    # Diabetes\n",
    "    (\"diabetes mellitus\", \"DM\", True),\n",
    "    (\"type 1 diabetes\", \"T1DM\", True),\n",
    "    (\"type 2 diabetes\", \"T2DM\", True),\n",
    "    (\"diabetic ketoacidosis\", \"DKA\", True),\n",
    "    \n",
    "    # Respiratory\n",
    "    (\"chronic obstructive pulmonary disease\", \"COPD\", True),\n",
    "    (\"shortness of breath\", \"SOB\", True),\n",
    "    (\"shortness of breath\", \"dyspnea\", True),\n",
    "    (\"pulmonary embolism\", \"PE\", True),\n",
    "    (\"acute respiratory distress syndrome\", \"ARDS\", True),\n",
    "    \n",
    "    # Neurological\n",
    "    (\"cerebrovascular accident\", \"CVA\", True),\n",
    "    (\"cerebrovascular accident\", \"stroke\", True),\n",
    "    (\"transient ischemic attack\", \"TIA\", True),\n",
    "    (\"traumatic brain injury\", \"TBI\", True),\n",
    "    (\"Glasgow Coma Scale\", \"GCS\", True),\n",
    "    \n",
    "    # Gastrointestinal\n",
    "    (\"gastroesophageal reflux disease\", \"GERD\", True),\n",
    "    (\"inflammatory bowel disease\", \"IBD\", True),\n",
    "    (\"irritable bowel syndrome\", \"IBS\", True),\n",
    "    \n",
    "    # Renal\n",
    "    (\"acute kidney injury\", \"AKI\", True),\n",
    "    (\"chronic kidney disease\", \"CKD\", True),\n",
    "    (\"end-stage renal disease\", \"ESRD\", True),\n",
    "    (\"urinary tract infection\", \"UTI\", True),\n",
    "    \n",
    "    # Vital signs\n",
    "    (\"blood pressure\", \"BP\", True),\n",
    "    (\"heart rate\", \"HR\", True),\n",
    "    (\"oxygen saturation\", \"SpO2\", True),\n",
    "    (\"respiratory rate\", \"RR\", True),\n",
    "    \n",
    "    # Symptoms\n",
    "    (\"chest pain\", \"CP\", True),\n",
    "    (\"fever\", \"pyrexia\", True),\n",
    "    (\"headache\", \"HA\", True),\n",
    "    \n",
    "    # Lab tests\n",
    "    (\"complete blood count\", \"CBC\", True),\n",
    "    (\"basic metabolic panel\", \"BMP\", True),\n",
    "    (\"prothrombin time\", \"PT\", True),\n",
    "    (\"international normalized ratio\", \"INR\", True),\n",
    "    \n",
    "    # Medications\n",
    "    (\"acetaminophen\", \"Tylenol\", True),\n",
    "    (\"ibuprofen\", \"Advil\", True),\n",
    "    (\"aspirin\", \"ASA\", True),\n",
    "    \n",
    "    # Procedures\n",
    "    (\"cardiopulmonary resuscitation\", \"CPR\", True),\n",
    "    (\"electrocardiogram\", \"ECG\", True),\n",
    "    (\"electrocardiogram\", \"EKG\", True),\n",
    "    (\"magnetic resonance imaging\", \"MRI\", True),\n",
    "    (\"computed tomography\", \"CT\", True),\n",
    "    \n",
    "    # Routes\n",
    "    (\"intravenous\", \"IV\", True),\n",
    "    (\"intramuscular\", \"IM\", True),\n",
    "    (\"by mouth\", \"PO\", True),\n",
    "    \n",
    "    # Negative cases\n",
    "    (\"hypertension\", \"hypotension\", False),\n",
    "    (\"hyperglycemia\", \"hypoglycemia\", False),\n",
    "    (\"tachycardia\", \"bradycardia\", False),\n",
    "    (\"STEMI\", \"NSTEMI\", False),\n",
    "    (\"ischemic stroke\", \"hemorrhagic stroke\", False),\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting {len(medical_tests)} medical terminology pairs...\")\n",
    "\n",
    "medical_results = []\n",
    "medical_failures = []\n",
    "\n",
    "for i, (text1, text2, should_match) in enumerate(medical_tests, 1):\n",
    "    result = comparator.compare(text1, text2)\n",
    "    matched = result.matched\n",
    "    confidence = result.confidence\n",
    "    \n",
    "    is_correct = matched == should_match\n",
    "    medical_results.append(is_correct)\n",
    "    \n",
    "    if not is_correct:\n",
    "        medical_failures.append((text1, text2, should_match, matched, confidence))\n",
    "    \n",
    "    if not is_correct or i % 10 == 0:\n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{status} [{i:3d}] '{text1}' vs '{text2}': {matched} (conf: {confidence:.3f})\")\n",
    "\n",
    "medical_accuracy = sum(medical_results) / len(medical_results)\n",
    "medical_std = np.std(medical_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MEDICAL DOMAIN RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {medical_accuracy:.4f} ({medical_accuracy*100:.2f}%)\")\n",
    "print(f\"Correct:   {sum(medical_results)}/{len(medical_results)}\")\n",
    "print(f\"Std Dev:   {medical_std:.4f}\")\n",
    "\n",
    "if medical_failures:\n",
    "    print(f\"\\nFailures ({len(medical_failures)}):\")\n",
    "    for text1, text2, should, got, conf in medical_failures[:5]:\n",
    "        print(f\"  '{text1}' vs '{text2}'\")\n",
    "        print(f\"    Expected: {should}, Got: {got}, Confidence: {conf:.3f}\")\n",
    "    if len(medical_failures) > 5:\n",
    "        print(f\"  ... and {len(medical_failures) - 5} more failures\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "legal-header"
   },
   "source": [
    "## 2.3 Legal Domain: ContractNLI & Legal Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "legal-test"
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"LEGAL DOMAIN: ContractNLI & Legal Citations\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nLoading ContractNLI dataset...\")\n",
    "\n",
    "# Load ContractNLI dataset\n",
    "try:\n",
    "    contractnli = load_dataset(\"coastalcph/lex_glue\", \"contractnli\", split=\"train[:500]\")\n",
    "    print(f\"✅ Loaded {len(contractnli)} examples from ContractNLI\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load ContractNLI: {e}\")\n",
    "    print(\"   Using hand-crafted legal examples instead...\")\n",
    "    contractnli = None\n",
    "\n",
    "# Test legal terminology paraphrases\n",
    "legal_tests = [\n",
    "    # USC citations\n",
    "    (\"42 USC § 1983\", \"Section 1983\", True),\n",
    "    (\"42 USC § 1983\", \"42 U.S.C. 1983\", True),\n",
    "    (\"42 USC § 1983\", \"42 United States Code Section 1983\", True),\n",
    "    (\"Title VII\", \"Title 7\", True),\n",
    "    (\"18 USC 1001\", \"18 U.S.C. § 1001\", True),\n",
    "    \n",
    "    # Latin legal terms\n",
    "    (\"habeas corpus\", \"writ of habeas corpus\", True),\n",
    "    (\"pro se\", \"self-represented\", True),\n",
    "    (\"pro bono\", \"for the public good\", True),\n",
    "    (\"voir dire\", \"jury selection\", True),\n",
    "    (\"prima facie\", \"at first sight\", True),\n",
    "    (\"res judicata\", \"matter adjudged\", True),\n",
    "    (\"stare decisis\", \"precedent\", True),\n",
    "    (\"amicus curiae\", \"friend of the court\", True),\n",
    "    (\"in camera\", \"in private\", True),\n",
    "    (\"ex parte\", \"one-sided\", True),\n",
    "    (\"de novo\", \"anew\", True),\n",
    "    (\"de facto\", \"in fact\", True),\n",
    "    (\"de jure\", \"by law\", True),\n",
    "    (\"per se\", \"by itself\", True),\n",
    "    (\"mens rea\", \"guilty mind\", True),\n",
    "    (\"actus reus\", \"guilty act\", True),\n",
    "    \n",
    "    # Court terminology\n",
    "    (\"certiorari\", \"cert\", True),\n",
    "    (\"writ of certiorari\", \"cert petition\", True),\n",
    "    (\"summary judgment\", \"SJ\", True),\n",
    "    (\"motion to dismiss\", \"MTD\", True),\n",
    "    (\"preliminary injunction\", \"PI\", True),\n",
    "    (\"temporary restraining order\", \"TRO\", True),\n",
    "    \n",
    "    # Parties\n",
    "    (\"plaintiff\", \"complainant\", True),\n",
    "    (\"plaintiff\", \"petitioner\", True),\n",
    "    (\"defendant\", \"respondent\", True),\n",
    "    (\"defendant\", \"accused\", True),\n",
    "    \n",
    "    # Civil law\n",
    "    (\"tort\", \"civil wrong\", True),\n",
    "    (\"negligence\", \"breach of duty\", True),\n",
    "    (\"breach of contract\", \"contract violation\", True),\n",
    "    (\"damages\", \"monetary compensation\", True),\n",
    "    (\"injunction\", \"court order\", True),\n",
    "    \n",
    "    # Criminal law\n",
    "    (\"beyond a reasonable doubt\", \"criminal standard\", True),\n",
    "    (\"preponderance of the evidence\", \"civil standard\", True),\n",
    "    (\"probable cause\", \"reasonable grounds\", True),\n",
    "    (\"Miranda rights\", \"right to remain silent\", True),\n",
    "    \n",
    "    # Legal proceedings\n",
    "    (\"deposition\", \"sworn testimony\", True),\n",
    "    (\"interrogatories\", \"written questions\", True),\n",
    "    (\"discovery\", \"evidence gathering\", True),\n",
    "    (\"subpoena\", \"court summons\", True),\n",
    "    (\"affidavit\", \"sworn statement\", True),\n",
    "    \n",
    "    # Constitutional law\n",
    "    (\"First Amendment\", \"freedom of speech\", True),\n",
    "    (\"Fourth Amendment\", \"search and seizure\", True),\n",
    "    (\"Fifth Amendment\", \"right against self-incrimination\", True),\n",
    "    (\"due process\", \"fair treatment\", True),\n",
    "    (\"equal protection\", \"equal treatment under law\", True),\n",
    "    \n",
    "    # Verdicts\n",
    "    (\"guilty verdict\", \"conviction\", True),\n",
    "    (\"not guilty verdict\", \"acquittal\", True),\n",
    "    (\"liability\", \"legal responsibility\", True),\n",
    "    (\"judgment\", \"court decision\", True),\n",
    "    (\"appeal\", \"review by higher court\", True),\n",
    "    (\"remand\", \"send back to lower court\", True),\n",
    "    (\"reverse\", \"overturn decision\", True),\n",
    "    (\"affirm\", \"uphold decision\", True),\n",
    "    \n",
    "    # Property law\n",
    "    (\"real property\", \"real estate\", True),\n",
    "    (\"personal property\", \"movable property\", True),\n",
    "    (\"easement\", \"right of way\", True),\n",
    "    \n",
    "    # Contract law\n",
    "    (\"consideration\", \"something of value\", True),\n",
    "    (\"offer and acceptance\", \"meeting of minds\", True),\n",
    "    (\"mutual assent\", \"agreement\", True),\n",
    "    \n",
    "    # Negative cases\n",
    "    (\"plaintiff\", \"defendant\", False),\n",
    "    (\"guilty\", \"not guilty\", False),\n",
    "    (\"civil\", \"criminal\", False),\n",
    "    (\"felony\", \"misdemeanor\", False),\n",
    "    (\"affirm\", \"reverse\", False),\n",
    "]\n",
    "\n",
    "print(f\"\\nTesting {len(legal_tests)} legal terminology pairs...\")\n",
    "\n",
    "legal_results = []\n",
    "legal_failures = []\n",
    "\n",
    "for i, (text1, text2, should_match) in enumerate(legal_tests, 1):\n",
    "    result = comparator.compare(text1, text2)\n",
    "    matched = result.matched\n",
    "    confidence = result.confidence\n",
    "    \n",
    "    is_correct = matched == should_match\n",
    "    legal_results.append(is_correct)\n",
    "    \n",
    "    if not is_correct:\n",
    "        legal_failures.append((text1, text2, should_match, matched, confidence))\n",
    "    \n",
    "    if not is_correct or i % 10 == 0:\n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{status} [{i:3d}] '{text1}' vs '{text2}': {matched} (conf: {confidence:.3f})\")\n",
    "\n",
    "legal_accuracy = sum(legal_results) / len(legal_results)\n",
    "legal_std = np.std(legal_results)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"LEGAL DOMAIN RESULTS\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Accuracy:  {legal_accuracy:.4f} ({legal_accuracy*100:.2f}%)\")\n",
    "print(f\"Correct:   {sum(legal_results)}/{len(legal_results)}\")\n",
    "print(f\"Std Dev:   {legal_std:.4f}\")\n",
    "\n",
    "if legal_failures:\n",
    "    print(f\"\\nFailures ({len(legal_failures)}):\")\n",
    "    for text1, text2, should, got, conf in legal_failures[:5]:\n",
    "        print(f\"  '{text1}' vs '{text2}'\")\n",
    "        print(f\"    Expected: {should}, Got: {got}, Confidence: {conf:.3f}\")\n",
    "    if len(legal_failures) > 5:\n",
    "        print(f\"  ... and {len(legal_failures) - 5} more failures\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final-results"
   },
   "source": [
    "# Final Results and Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "final-decision"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL COMPREHENSIVE RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nGeneral Text (STS-Benchmark):\")\n",
    "print(f\"  Accuracy: {sts_accuracy:.4f} ({sts_accuracy*100:.2f}%)\")\n",
    "print(f\"  Samples:  2,879 pairs\")\n",
    "\n",
    "print(\"\\nDomain-Specific Results:\")\n",
    "print(f\"  Financial: {financial_accuracy:.4f} ({financial_accuracy*100:.2f}%)\")\n",
    "print(f\"  Medical:   {medical_accuracy:.4f} ({medical_accuracy*100:.2f}%)\")\n",
    "print(f\"  Legal:     {legal_accuracy:.4f} ({legal_accuracy*100:.2f}%)\")\n",
    "\n",
    "avg_domain = (financial_accuracy + medical_accuracy + legal_accuracy) / 3\n",
    "domain_std = np.std([financial_accuracy, medical_accuracy, legal_accuracy])\n",
    "\n",
    "print(f\"\\nDomain Average: {avg_domain:.4f} ({avg_domain*100:.2f}%) ± {domain_std:.4f}\")\n",
    "print(f\"Domain Std Dev: {domain_std:.4f} ({domain_std*100:.2f}%)\")\n",
    "\n",
    "# Overall average\n",
    "overall_avg = (sts_accuracy + avg_domain) / 2\n",
    "print(f\"\\nOverall Average: {overall_avg:.4f} ({overall_avg*100:.2f}%)\")\n",
    "\n",
    "# Decision framework\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL DECISION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nGeneral Text (STS-Benchmark): {sts_accuracy*100:.1f}%\")\n",
    "if sts_accuracy >= 0.85:\n",
    "    print(\"  ✅ Excellent performance on general semantic similarity\")\n",
    "elif sts_accuracy >= 0.75:\n",
    "    print(\"  ⚠️  Good performance, may need tuning\")\n",
    "else:\n",
    "    print(\"  ❌ Below target, consider alternative models\")\n",
    "\n",
    "print(f\"\\nDomain-Specific (Financial/Medical/Legal): {avg_domain*100:.1f}%\")\n",
    "if avg_domain >= 0.85:\n",
    "    print(\"  ✅ SHIP IT: Excellent domain performance\")\n",
    "    print(\"     Embeddings handle domain terminology well.\")\n",
    "    print(\"     No fine-tuning needed.\")\n",
    "    recommendation = \"SHIP\"\n",
    "elif avg_domain >= 0.75:\n",
    "    print(\"  ⚠️  CONSIDER TRAINING: Good but not excellent\")\n",
    "    print(\"     Embeddings work, training could improve 5-10%.\")\n",
    "    print(\"     Decide based on production requirements.\")\n",
    "    recommendation = \"CONSIDER\"\n",
    "else:\n",
    "    print(\"  ❌ TRAIN: Below target on domain-specific data\")\n",
    "    print(\"     Domain-specific fine-tuning recommended.\")\n",
    "    print(\"     Embeddings struggle with specialized jargon.\")\n",
    "    recommendation = \"TRAIN\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RECOMMENDATION: {recommendation}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nKey Statistics:\")\n",
    "print(f\"  STS-Benchmark:     {sts_accuracy*100:.2f}%\")\n",
    "print(f\"  Financial Domain:  {financial_accuracy*100:.2f}%\")\n",
    "print(f\"  Medical Domain:    {medical_accuracy*100:.2f}%\")\n",
    "print(f\"  Legal Domain:      {legal_accuracy*100:.2f}%\")\n",
    "print(f\"  Domain Average:    {avg_domain*100:.2f}% ± {domain_std*100:.2f}%\")\n",
    "print(f\"  Overall Average:   {overall_avg*100:.2f}%\")\n",
    "\n",
    "if recommendation == \"SHIP\":\n",
    "    print(\"\\n✅ Embeddings meet all criteria. Ready for production.\")\n",
    "elif recommendation == \"CONSIDER\":\n",
    "    print(\"\\n⚠️  Embeddings are viable. Consider training for marginal gains.\")\n",
    "else:\n",
    "    print(\"\\n❌ Domain-specific training will significantly improve performance.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided comprehensive validation:\n",
    "\n",
    "1. **STS-Benchmark**: Baseline performance on general text similarity\n",
    "2. **Domain-Specific**: Real-world performance on Financial, Medical, and Legal terminology\n",
    "3. **Complete Statistics**: Accuracy, precision, recall, F1, standard deviation\n",
    "4. **Clear Decision**: SHIP / CONSIDER / TRAIN based on measured performance\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- If **SHIP**: Deploy embeddings as-is, monitor production metrics\n",
    "- If **CONSIDER**: Collect production data, measure actual improvement from training\n",
    "- If **TRAIN**: Use domain-specific datasets to fine-tune embeddings\n",
    "\n",
    "### References:\n",
    "\n",
    "- STS-Benchmark: Cer et al. (2017)\n",
    "- FinQA: Chen et al. (2021)\n",
    "- PubMedQA: Jin et al. (2019)\n",
    "- ContractNLI: Koreeda & Manning (2021)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "CERT Complete Validation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
